{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/logo_nao_digital.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 1: Elementos de Deep Learning\n",
    "## 1. Objetivo\n",
    "\n",
    "Familiarizarse con los conceptos de Deep Learning para plantear un modelo que permita crear un ChatBot de conversación en texto.\n",
    "\n",
    "## Datos de la Medium Amira Rashid\n",
    "\n",
    "Cómo se ha mencionado en el Anexo A, se ha provisto el archivo `conversations.json` que contiene los scripts de interacciones de usuarios y las respuestas que se espera que reciban, mismos que han sido provistos por el equipo de Amira a la empresa ChatBot Intelligence. \n",
    "\n",
    "Esencialmente se trata de una estructura que contiene una serie de scripts de conversación:\n",
    "* **tag:** es una etiqueta que clasifica el tipo de interacción entre el usuario y el Chatbot (por ejemplo, “saludo”, “despedida”, etcétera),\n",
    "* **patterns:** conversaciones de ejemplo que se espera recibir como consultar al chatbot por parte de los usuarios,\n",
    "* **responses:** es una lista de las respuestas en texto que se espera que los usuarios reciban.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este documento se desarrollarán scripts en Python que permitan plantear un primer modelo predictivo en el framework de Deep Learning para generar un ChatBot de conversación."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Librerias de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala libreria Pandas si no la tenemos\n",
    "#!pip install numpy tensorflow pandas matplotlib seaborn wordcloud scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instalación de Spacy**\n",
    "\n",
    "Spacy provee en su documentación las instrucciones para instalar su librería: https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U pip setuptools wheel\n",
    "#pip install -U spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "#python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Elementos de Deep Learning\n",
    "\n",
    "El Aprendizaje Profundo (Deep Learning) es una rama de la inteligencia artificial que se enfoca en entrenar modelos inspirados en redes neuronales del cerebro y que han cobrado gran relevancia en las últimas décadas al aprender a realizar tareas complejas como la visión por computadora y el procesamiento del lenguaje natural.\n",
    "\n",
    "En esta sección se abordarán elementos básicos de Deep Learnign que permitiran plantear modelos predictivaos para la construcción de un Chatbot.\n",
    "\n",
    "### 3.1 Modelos de redes neuronales\n",
    "\n",
    "Primero comencemos recordando que en los modelos de regresión lineal, la idea esencial que para el conjunto de características del fenómeno $X = (x_1, \\ldots, x_n)$ y la variable  objetivo $Y$ que queremos predecir existe una colección de números reales $W= (w_1, \\ldots, w_n)$ y un sesgo $b \\in \\mathbb{R}$ de forma que podemos aproximar\n",
    "\n",
    "$$Y \\approx w_1 \\cdot x_1 + \\ldots + w_n \\cdot x_n + b $$\n",
    "\n",
    "De hecho, en los modelos de regresión generalizados, como la regresión logística, se asume la existena de una función $\\mu$ especial (biyectiva, no lineal y generalmente suave) tal que:\n",
    "\n",
    "$$Y \\approx \\mu^{-1}(w_1 \\cdot x_1 + \\ldots + w_n \\cdot x_n + b) $$\n",
    "\n",
    "La idea en estos casos es que si bien $Y$ puede ser un fenómeno complejo y no lineal, la forma funcional (pesos calibrados de alguna forma más un sesgo) de una regresión y el efecto de una función no lineal adecuada puede ayudar a explicar y predecir el fenómeno aproximadamente bien.\n",
    "\n",
    "### 3.1.1 Perceptrón\n",
    "\n",
    "De hecho, el modelo más básico de redes neuronales, llamado **Perceptrón**, es una generalización de las ideas recién descritas, pues en él nuevamente contamos con un conjunto de datos con características numéricas $X = (x_1, \\ldots, x_n)$ y una variable objetivo $Y$, en que el modelo aproxima al objetivo con ciertos pesos $(w_1, \\ldots, w_n)$ y un sesgo $b$, y la predicción final se obtiene aplicando una función de activación como en la ecuación siguente:\n",
    "\n",
    "$$Y \\approx activation(w_1 \\cdot x_1 + \\ldots + w_n \\cdot x_n + b) $$\n",
    "\n",
    "\n",
    "![title](../images/perceptron.png)\n",
    "\n",
    "Claramente como en cualquier problema de aprendizaje supervisado, necesitamos definir como medir el error y como encontrar el valor adecuado de los parámetros involucrados, es decir, es necesario:\n",
    "\n",
    "* Una colección de datos con características $X_1, \\ldots X_n$ y una varible objetivo $Y$ a predecirse,\n",
    "* El modelo que es una función  $f$ que aproxima el valor de $Y$ a partir de las características  $X_1, \\ldots X_n$, definida con un conjunto de parámetros $\\Theta:= (w_1, \\ldots, w_n,b)$. Es decir:\n",
    "  * $f(X_1, \\ldots X_n, \\Theta) \\approx Y$\n",
    "* Una forma de medir el error entre la predicción del modelo, esta se conoce típicamente como función de pérdida y esencialmente es una representación matemática que nos permite saber cuanto dista la predicción del valor real de la etiqueta:\n",
    "\n",
    "  $$ l(f(X_1, \\ldots X_n, \\Theta), Y) $$\n",
    "* Alguna técnica algorítmica que permita ajustar los valores de los parámetros $\\Theta$ hasta  encontra un valor donde se alcance el mínimo error posible, es decir, minimizar la función de pérdida \n",
    "\n",
    "$$ \\min_{\\Theta} l(f(X_1, \\ldots X_n, \\Theta), Y)  $$\n",
    "\n",
    "### 3.1.2 Modelos más generales.\n",
    "\n",
    "El poder de las redes neuronales es que son modelos muy flexibles  en el que se puede adaptar su estructura de funcionamiento para añaidr más unidades como el perceptrón dentro de su estructura de forma que permiten retroalimentar las estimaciones que el modelo realiza. Esto les permite:\n",
    "\n",
    "* Conectar capas a una misma unidad de datos de entrada,\n",
    "* Conectar multiples capas de perceptrones que se alimenten a su vez de capas anteriores,\n",
    "* Recibir datos de entrada con diferentes estructuras numéricas, como valores escalares, vectores o arreglos matriciales, que los modelos clásicos de Aprendizaje de Máquina no pueden lograr pues únicamente se limitan a estructuras de tablas,\n",
    "* Predecir datos de salida con diferentes estructuras numéricas, como valores escalares, vectores o arreglos matriciales, que nuevamene los modelos clásicos de Aprendizaje de Máquina no pueden.\n",
    "\n",
    "El término de **aprendizaje profundo** para estos modelos hace referencia a las diversas capas de redes neuronales que se acumulan en su estrucrtura y el rendimiento mejora a medida que la red crece en profundidad. \n",
    "\n",
    "Éstas ventajas son parte de las fortalezas de los modelos que les permiten resolver problemas de gran complejidad. Particularmente en el caso de problemas de Procesamiento de Lenguaje Natural, se trata de predecir cadenas de texto, es decir, sequencias con un orden específico, por lo que los modelos de esta clase pueden asumir el reto sin problemas.\n",
    "\n",
    "![title](../images/dl_layer.png)\n",
    "\n",
    "### 3.1.3 ¿Cómo calibrar los pesos?\n",
    "\n",
    "Retornando al ejemplo de esta discusión donde se abordó la analogía con los modelos de regresión, los coeficiente de una relación funcional del estilo\n",
    "$$Y \\approx w_1 \\cdot x_1 + \\ldots + w_n \\cdot x_n + b $$\n",
    "\n",
    "Se suelen estimar con ciertas hipótesis estadísticas y métodos numéricos. De entre éstos últimos métodos, destaca un algoritmo denominado **descenso de gradiente**, que en términos simple es un algoritmo recursivo para encontrar el valor de un conjunto de parámetros donde la función de pérdida miniza su valor y con ello permite encontrar los valores de un modelo en donde los parámetros generan menor error.\n",
    "\n",
    "Su idea básicamente es tomar un punto de inicio y contruir un nuevo punto para evaluar si la función de pérdida tiene un valor más pequeño; la estrategia se base en calcular la derivada de la función (que en muchas dimensiones se conoce como gradiente), para encontrar la dirección y la magnitud del cambio más pronunciado en la función, es decir, donde la función debería tomar valores más pequeño y tomar un punto la dirección opuesta al gradiente. En otras palabras, si el gradiente es positivo, el algoritmo ajustará los parámetros en la dirección negativa para reducir la función de costo, y si el gradiente es negativo, el algoritmo ajustará los parámetros en la dirección positiva.\n",
    "\n",
    "Este proceso se repite iterativamente hasta que se alcanza un mínimo de la función de pérdida, lo que significa que los parámetros del modelo están optimizados para producir las mejores predicciones posibles.\n",
    "\n",
    "El tamaño del paso que se toma en cada iteración se conoce como la **tasa de aprendizaje**. Una tasa de aprendizaje alta puede hacer que el algoritmo converja rápidamente, pero puede ser propensa a saltar sobre el mínimo global, mientras que una tasa de aprendizaje baja puede hacer que el algoritmo converja lentamente, pero puede encontrar el mínimo global de manera más confiable.\n",
    "\n",
    "Existe una versión de éste algoritmo que puede lidiar con grandes volumenes de datos sacrificando precisión en las estimaciones, conocido como **descenso de gradiente estocástico** ya que  actualiza los parámetros del modelo para cada ejemplo de entrenamiento individual y que también se puede adaptar para usar lotes de individuos para entrenar en lugar de todo el conjunto original de datos.\n",
    "\n",
    "Ahora bien, dado que normalmente los modelos de Deep Learning tienen expresiones complejas, el cálculo del gradiente puede tener una expresión compleja (básicamente es la regla de la cadena para funciones de dimensiones altas), los algoritmos numéricos de optimización emplean el proceso llamado **Propagación hacia atrás** (Back propagation, por sus siglas en inglés), donde se ajustas cada uno los valores de pesos de las conexiones entre las neuronas de la red neuronal para minimizar el error en la salida de la red.\n",
    "\n",
    "El proceso de backpropagation implica calcular el gradiente de la función de error con respecto a los pesos de las conexiones en la red neuronal, y luego ajustar los pesos en la dirección opuesta al gradiente para minimizar el error. Este proceso se repite iterativamente para cada ejemplo en el conjunto de entrenamiento hasta que se alcanza un punto de convergencia. Aquí cobra relevancia el concepto de **época**, que se refiere a la cantidad de iteraciones completas que se hacen para ajustar los pesos considerando a todos los individuos del conjunto de entrenamiento. La teoria dice que aumentar la cantidad de épocas puede mejorar el ajuste del modelo, pero un número grande puede llevar a sobre entrenamiento.\n",
    "\n",
    "### 3.1.4 Tipos de capas en modelos de aprendizaje profundo\n",
    "\n",
    "En el contexto de redes se usa cierta terminología para referirse a las capas de una red:\n",
    "\n",
    "* **Capa Densa:** es un tipo de capa de una red neuronal donde cada neurona de una capa está conectada a todas las neuronas de la capa siguiente, tambiñen se llama capa totalmente conectada,\n",
    "* **Capa Droput:** se refiere a una capa que responde a una técnica de regularización, donde se desactivan aleatoriamente algunas neuronas (ude una capa durante el entrenamiento del modelo. La idea detrás de esta técnica es prevenir el sobreajuste (overfitting) del modelo al reducir la dependencia entre las neuronas y evitar que el modelo memorice el conjunto de datos de entrenamiento en lugar de generalizar a nuevos datos.\n",
    "\n",
    "### 3.1.5 Otros puntos a considerar\n",
    "\n",
    "Hay otros puntos relevantes a mencionar:\n",
    "\n",
    "* Al sumar muchas capaz a la estructura de un modelo de aprendizaje profundo la cantidad de parámetros crecen muy rápido,\n",
    "* En consecuencia, pueden necesitan una cantidad muy amplia de datos para entrenarse,\n",
    "* Al ser tan flexibles, pueden sufrir sobre ajuste,\n",
    "* La arquitectura de una red que sirve para resolver un problema específico no es conocida de entrada, se necesita experimentar y combinar ingeniería de características para garantizar el éxito de un modelo, así como contar con un volumen de datos adecuado junto con suficiente poder predictivo,\n",
    "* Existe diferentes funciones de activación que se han probado con éxito en distintos contextos, entre ellas destacan las funciones *sigmoide*, *softmax*, *ReLU* y otras relacionada con funciones trigonométricas hiperbólicas (ver https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253),\n",
    "* Como en cualquier problema de aprendizaje, un parte importante es definir adecuadamente la función de pérdida para resolver el problema."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Usando TensorFlow y Keras\n",
    "\n",
    "TensorFlow es una biblioteca de software, creada por Google, que permite a los desarrolladores construir modelos de redes neuronales y otros modelos de aprendizaje automático, con una sintaxis fácil de entender y que facilita el proceso de entrenamiento y puesta en producción de un modelo. Por su parte, Keras es una interfaz de programación de aplicaciones (API) que permite a los desarrolladores crear y entrenar modelos de aprendizaje profundo , a través de la abstracción códifo de nivel superior que facilita la construcción de tales modelos.\n",
    "\n",
    "Existen otros frameworks de entrenamiento de modelos de Deep Learning, como PyTorch (https://pytorch.org), que basan su sintaxis en programación orientada a objectos, lo que puede implicar un nivel de complejidad más alto para implementar modelos de redes neuronales.\n",
    "\n",
    "A continuación veremos algunos ejemplos de código sobre como entrenar modelos de aprendizaje profundo usando ambas herramientas\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Ejemplo: Entrenando una red neuronal sobre el conjunto de datos Iris\n",
    "\n",
    "El conjunto de datos de flores Iris es un conjunto de datos famoso que contiene información sobre tres diferentes especies de flores de iris: Setosa, Versicolor y Virginica.\n",
    "\n",
    "Cada muestra en el conjunto de datos representa una flor de iris, y hay un total de 150 muestras. Cada muestra tiene cuatro características: longitud del sépalo, ancho del sépalo, longitud del pétalo y ancho del pétalo.\n",
    "\n",
    "Entrenaremos una red neuronal para predecir el tipo de flor con base en sus características físicas.\n",
    "\n",
    "**Importa librerías relevantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Dummy Data from sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargamos el conjunto de datos**\n",
    "\n",
    "Usamos la función `load_iris` de Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el conjunto de datos iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos valores corresponden a las columnas `sepal width (cm)`, `petal length (cm)` y `petal width (cm)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "101                5.8               2.7                5.1               1.9\n",
       "38                 4.4               3.0                1.3               0.2\n",
       "70                 5.9               3.2                4.8               1.8\n",
       "136                6.3               3.4                5.6               2.4\n",
       "145                6.7               3.0                5.2               2.3\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "86                 6.7               3.1                4.7               1.5\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "129                7.2               3.0                5.8               1.6\n",
       "80                 5.5               2.4                3.8               1.1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iris.data, columns=iris['feature_names']).sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También tenemos el tipo de flor al que corresponden (`Versicolor = 0`, `Virginia=1`, `Setosa = 2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type\n",
       "82      1\n",
       "51      1\n",
       "107     2\n",
       "91      1\n",
       "131     2\n",
       "129     2\n",
       "111     2\n",
       "2       0\n",
       "99      1\n",
       "34      0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iris.target, columns=['type']).sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separamos los datos en conjuntos de entrenamiento en prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide el conjunto de datos en conjuntos de train y testr\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usamos el API de Keras para definir el modelo**\n",
    "\n",
    "Keras permite una definición sencilla de modelo a través de su función `Sequential`, la cual acepta diferentes tipos de capas que van definiendo la arquitectura de la red.\n",
    "\n",
    "En el ejemplo inferior tenemos que:\n",
    "* La primera capa es de tipo denso con 10 unidades de perceptron y una función de activación sigmoide, teniendo como datos de entrada a vectores de tamaño 4 (por las 4 columnas del conjunto Iris).\n",
    "* La segunda capas es del tipo denso pero reduce su tamaño a 10 unidades y emplea una función de activación `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un modelo secuencial de TensorFlow\n",
    "model = Sequential([\n",
    "    Dense(10, activation='sigmoid', input_shape=(4,)),\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compilando del modelo**\n",
    "\n",
    "Esta esta se refiere a la compilación del modelo recién creado en donde especificamos 1) la función de pérdida que queremos usar, 2) el tipo de optimizador (SGD, en nuestro caso) y 3) la métrica que se va a emplear para medir el performance del modelo (al ser un problema de clasificación, aquí se usa una métrica relevante como **accuracy**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila el modelo\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ajustando el modelo**\n",
    "\n",
    "En esta etapa el modelo se ajusta con el método `.fit()` sobre los datos de entrenamiento, en este caso se espeficón un ajuste de 50 iteraciones sobre los datos de entrenamiento (es decir, 50 épocas), usando lotes de tamaño 5.\n",
    "\n",
    "El parámetro `verbose=1` nos imprime la infomación de la función de pérdida y el error obtenido en la iteración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.4585 - accuracy: 0.3500\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.2083 - accuracy: 0.3500\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.1174 - accuracy: 0.3500\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 944us/step - loss: 1.0809 - accuracy: 0.3500\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 986us/step - loss: 1.0634 - accuracy: 0.3500\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0515 - accuracy: 0.3667\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0418 - accuracy: 0.6333\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0333 - accuracy: 0.6833\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 0s 971us/step - loss: 1.0263 - accuracy: 0.6917\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0179 - accuracy: 0.6917\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0096 - accuracy: 0.6917\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 0s 945us/step - loss: 1.0023 - accuracy: 0.6917\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9943 - accuracy: 0.6917\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9865 - accuracy: 0.6917\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9782 - accuracy: 0.6917\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 0s 948us/step - loss: 0.9709 - accuracy: 0.6917\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9622 - accuracy: 0.6917\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9540 - accuracy: 0.6917\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9457 - accuracy: 0.6917\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.9376 - accuracy: 0.6917\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 0s 935us/step - loss: 0.9294 - accuracy: 0.6917\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9209 - accuracy: 0.6917\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9113 - accuracy: 0.6917\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9033 - accuracy: 0.6917\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8955 - accuracy: 0.6917\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 0s 998us/step - loss: 0.8861 - accuracy: 0.6917\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8778 - accuracy: 0.6917\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8693 - accuracy: 0.6917\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8609 - accuracy: 0.6917\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8517 - accuracy: 0.6917\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8436 - accuracy: 0.6917\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8349 - accuracy: 0.6917\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 0s 941us/step - loss: 0.8259 - accuracy: 0.6917\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8176 - accuracy: 0.6917\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.8100 - accuracy: 0.6917\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 0s 961us/step - loss: 0.8018 - accuracy: 0.6917\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.7929 - accuracy: 0.6917\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.7845 - accuracy: 0.6917\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 0s 927us/step - loss: 0.7769 - accuracy: 0.6917\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 0s 984us/step - loss: 0.7694 - accuracy: 0.6917\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 0s 941us/step - loss: 0.7612 - accuracy: 0.7083\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 0s 952us/step - loss: 0.7531 - accuracy: 0.6917\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 0s 933us/step - loss: 0.7467 - accuracy: 0.6917\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7388 - accuracy: 0.6917\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7309 - accuracy: 0.6917\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 0s 941us/step - loss: 0.7232 - accuracy: 0.6917\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7176 - accuracy: 0.7000\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 0s 910us/step - loss: 0.7097 - accuracy: 0.6917\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7030 - accuracy: 0.6917\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6958 - accuracy: 0.6917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrena el modelo\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=5,\n",
    "    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluando el performance del modelo**\n",
    "\n",
    "Con el modelo ya entrenamos, podemos probar el performance con la métrica que especificamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.5666666626930237\n"
     ]
    }
   ],
   "source": [
    "# Evalúa el modelo con el conjunto de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Imprime la precisión del modelo\n",
    "print('Precisión del modelo:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos analizar como cambian estos valores a lo largo de lás épocas de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABR/UlEQVR4nO3deVxU9f4/8NcwA8MmA7JvCgLiCu6E+4KRFT81K20DtaxMy1y6aqVm3Rt+u+nDFsu695p5M7UszdK6Gu77ikuiArIosors+8z5/XFkcAQRcGYOM7yej8d5AGeZec+51bzu53wWmSAIAoiIiIjMhIXUBRARERHpE8MNERERmRWGGyIiIjIrDDdERERkVhhuiIiIyKww3BAREZFZYbghIiIis8JwQ0RERGaF4YaIiIjMCsMNEelNamoqZDIZ1q5d2+xr9+7dC5lMhr179+q9LiJqWxhuiIiIyKww3BAREZFZYbghIjKg0tJSqUsganMYbojMyHvvvQeZTIYrV67g+eefh0qlgqurKxYtWgRBEHDt2jWMHTsWDg4O8PDwwPLly+u9Rk5ODl588UW4u7vD2toaoaGh+Pbbb+udV1BQgMmTJ0OlUsHR0RExMTEoKChosK5Lly7hySefRPv27WFtbY1+/fph27ZtLfqMaWlpeO211xAcHAwbGxs4OzvjqaeeQmpqaoM1zp49G35+flAqlfDx8UF0dDTy8vK051RUVOC9995D586dYW1tDU9PTzzxxBNITk4GcO++QA31L5o8eTLs7e2RnJyMRx99FO3atcNzzz0HADhw4ACeeuopdOjQAUqlEr6+vpg9ezbKy8sbvF9PP/00XF1dYWNjg+DgYLzzzjsAgD179kAmk2HLli31rvv+++8hk8lw5MiR5t5WIrOikLoAItK/iRMnomvXrli2bBm2b9+Ov//972jfvj2++uorjBw5Ev/3f/+H9evXY968eejfvz+GDh0KACgvL8fw4cORlJSEmTNnwt/fHz/++CMmT56MgoICzJo1CwAgCALGjh2LgwcP4tVXX0XXrl2xZcsWxMTE1Kvlr7/+wqBBg+Dt7Y0FCxbAzs4OP/zwA8aNG4effvoJ48ePb9ZnO3HiBA4fPoxJkybBx8cHqamp+PLLLzF8+HBcvHgRtra2AICSkhIMGTIECQkJmDp1Kvr06YO8vDxs27YN169fh4uLC9RqNR5//HHExcVh0qRJmDVrFoqLi7Fr1y5cuHABAQEBzb73NTU1iIyMxODBg/Hxxx9r6/nxxx9RVlaG6dOnw9nZGcePH8dnn32G69ev48cff9Ref+7cOQwZMgSWlpZ4+eWX4efnh+TkZPz666/4xz/+geHDh8PX1xfr16+vd+/Wr1+PgIAAhIeHN7tuIrMiEJHZWLJkiQBAePnll7X7ampqBB8fH0EmkwnLli3T7r9165ZgY2MjxMTEaPetXLlSACB899132n1VVVVCeHi4YG9vLxQVFQmCIAhbt24VAAgfffSRzvsMGTJEACB888032v2jRo0SevbsKVRUVGj3aTQaYeDAgUJQUJB23549ewQAwp49exr9jGVlZfX2HTlyRAAgrFu3Trtv8eLFAgDh559/rne+RqMRBEEQ1qxZIwAQVqxYcc9z7lVXSkpKvc8aExMjABAWLFjQpLpjY2MFmUwmpKWlafcNHTpUaNeunc6+O+sRBEFYuHChoFQqhYKCAu2+nJwcQaFQCEuWLKn3PkRtDR9LEZmhl156Sfu7XC5Hv379IAgCXnzxRe1+R0dHBAcH4+rVq9p9O3bsgIeHB5555hntPktLS7zxxhsoKSnBvn37tOcpFApMnz5d531ef/11nTry8/Oxe/duPP300yguLkZeXh7y8vJw8+ZNREZGIjExERkZGc36bDY2Ntrfq6urcfPmTQQGBsLR0RGnT5/WHvvpp58QGhraYMuQTCbTnuPi4lKv7jvPaYk770tDdZeWliIvLw8DBw6EIAg4c+YMACA3Nxf79+/H1KlT0aFDh3vWEx0djcrKSmzevFm7b9OmTaipqcHzzz/f4rqJzAXDDZEZuvuLUaVSwdraGi4uLvX237p1S/t3WloagoKCYGGh+5+Grl27ao/X/vT09IS9vb3OecHBwTp/JyUlQRAELFq0CK6urjrbkiVLAIh9fJqjvLwcixcvhq+vL5RKJVxcXODq6oqCggIUFhZqz0tOTkaPHj0afa3k5GQEBwdDodDfE3qFQgEfH596+9PT0zF58mS0b98e9vb2cHV1xbBhwwBAW3dt0Lxf3V26dEH//v2xfv167b7169fjoYceQmBgoL4+CpHJYp8bIjMkl8ubtA8Q+88YikajAQDMmzcPkZGRDZ7T3C/j119/Hd988w3efPNNhIeHQ6VSQSaTYdKkSdr306d7teCo1eoG9yuVynrhUK1WY/To0cjPz8f8+fPRpUsX2NnZISMjA5MnT25R3dHR0Zg1axauX7+OyspKHD16FJ9//nmzX4fIHDHcEJFWx44dce7cOWg0Gp0v6EuXLmmP1/6Mi4tDSUmJTuvN5cuXdV6vU6dOAMRHWxEREXqpcfPmzYiJidEZ6VVRUVFvpFZAQAAuXLjQ6GsFBATg2LFjqK6uhqWlZYPnODk5AUC9169txWqK8+fP48qVK/j2228RHR2t3b9r1y6d82rv1/3qBoBJkyZhzpw52LBhA8rLy2FpaYmJEyc2uSYic8bHUkSk9eijjyIrKwubNm3S7qupqcFnn30Ge3t77WOURx99FDU1Nfjyyy+156nVanz22Wc6r+fm5obhw4fjq6++QmZmZr33y83NbXaNcrm8XmvTZ599Vq8lZcKECTh79myDQ6Zrr58wYQLy8vIabPGoPadjx46Qy+XYv3+/zvEvvviiWTXf+Zq1v3/yySc657m6umLo0KFYs2YN0tPTG6ynlouLC8aMGYPvvvsO69evxyOPPFLvsSNRW8WWGyLSevnll/HVV19h8uTJOHXqFPz8/LB582YcOnQIK1euRLt27QAAUVFRGDRoEBYsWIDU1FR069YNP//8s06fl1qrVq3C4MGD0bNnT0ybNg2dOnVCdnY2jhw5guvXr+Ps2bPNqvHxxx/Hf//7X6hUKnTr1g1HjhzBn3/+CWdnZ53z3nrrLWzevBlPPfUUpk6dir59+yI/Px/btm3D6tWrERoaiujoaKxbtw5z5szB8ePHMWTIEJSWluLPP//Ea6+9hrFjx0KlUuGpp57CZ599BplMhoCAAPz222/N6ivUpUsXBAQEYN68ecjIyICDgwN++uknnf5OtT799FMMHjwYffr0wcsvvwx/f3+kpqZi+/btiI+P1zk3OjoaTz75JADggw8+aNZ9JDJrUg3TIiL9qx0Knpubq7M/JiZGsLOzq3f+sGHDhO7du+vsy87OFqZMmSK4uLgIVlZWQs+ePXWGO9e6efOm8MILLwgODg6CSqUSXnjhBeHMmTP1hkcLgiAkJycL0dHRgoeHh2BpaSl4e3sLjz/+uLB582btOU0dCn7r1i1tffb29kJkZKRw6dIloWPHjjrD2mtrnDlzpuDt7S1YWVkJPj4+QkxMjJCXl6c9p6ysTHjnnXcEf39/wdLSUvDw8BCefPJJITk5WXtObm6uMGHCBMHW1lZwcnISXnnlFeHChQsNDgVv6D4LgiBcvHhRiIiIEOzt7QUXFxdh2rRpwtmzZxu8XxcuXBDGjx8vODo6CtbW1kJwcLCwaNGieq9ZWVkpODk5CSqVSigvL2/0vhG1JTJBMGBvQiIiMpiamhp4eXkhKioK//nPf6Quh6jVYJ8bIiITtXXrVuTm5up0UiYigC03REQm5tixYzh37hw++OADuLi46ExeSERsuSEiMjlffvklpk+fDjc3N6xbt07qcohaHbbcEBERkVlhyw0RERGZFYYbIiIiMittbhI/jUaDGzduoF27dg+06i8REREZjyAIKC4uhpeXV7312+7W5sLNjRs34OvrK3UZRERE1ALXrl2Dj49Po+e0uXBTO338tWvX4ODgIHE1RERE1BRFRUXw9fXVfo83ps2Fm9pHUQ4ODgw3REREJqYpXUrYoZiIiIjMCsMNERERmRWGGyIiIjIrba7PTVOp1WpUV1dLXYZJsrS0hFwul7oMIiJqoxhu7iIIArKyslBQUCB1KSbN0dERHh4enEuIiIiMjuHmLrXBxs3NDba2tvxybiZBEFBWVoacnBwAgKenp8QVERFRW8Nwcwe1Wq0NNs7OzlKXY7JsbGwAADk5OXBzc+MjKiIiMip2KL5DbR8bW1tbiSsxfbX3kP2WiIjI2BhuGsBHUQ+O95CIiKTCcENERERmheGG6vHz88PKlSulLoOIiKhF2KHYTAwfPhy9evXSSyg5ceIE7OzsHrwoIiIiCTDc6IsgAJoaQKMGLK2lrqYeQRCgVquhUNz/f3JXV1cjVERERGQYfCylL5XFQPYF4FaK0d968uTJ2LdvHz755BPIZDLIZDKsXbsWMpkMv//+O/r27QulUomDBw8iOTkZY8eOhbu7O+zt7dG/f3/8+eefOq9392MpmUyGf//73xg/fjxsbW0RFBSEbdu2GflTEhERNQ3DzX0IgoCyqpr7b2o5yqo1KKuobNr5TdgEQWhSjZ988gnCw8Mxbdo0ZGZmIjMzE76+vgCABQsWYNmyZUhISEBISAhKSkrw6KOPIi4uDmfOnMEjjzyCqKgopKenN/oeS5cuxdNPP41z587h0UcfxXPPPYf8/PwHvr9ERET6xsdS91FerUa3xf9r5lUZennvi+9Hwtbq/v8TqVQqWFlZwdbWFh4eHgCAS5cuAQDef/99jB49Wntu+/btERoaqv37gw8+wJYtW7Bt2zbMnDnznu8xefJkPPPMMwCADz/8EJ9++imOHz+ORx55pEWfjYiIyFDYcmPm+vXrp/N3SUkJ5s2bh65du8LR0RH29vZISEi4b8tNSEiI9nc7Ozs4ODhol1ggIiJqTdhycx82lnJcfD+yaSfnXQGqywFHf8DGQS/v/aDuHvU0b9487Nq1Cx9//DECAwNhY2ODJ598ElVVVY2+jqWlpc7fMpkMGo3mgesjIiLSN4ab+5DJZE16NAQAsLYGUAnI1UBTr9ETKysrqNXq+5536NAhTJ48GePHjwcgtuSkpqYauDoiIiLj4WMpfZIrxZ/qxltBDMHPzw/Hjh1Damoq8vLy7tmqEhQUhJ9//hnx8fE4e/Ysnn32WbbAEBGRWWG40Sf57Uc3EoSbefPmQS6Xo1u3bnB1db1nH5oVK1bAyckJAwcORFRUFCIjI9GnTx8jV0tERGQ4MqGp443NRFFREVQqFQoLC+HgoNsvpqKiAikpKfD394e1dQsm4isvEOe5sbQFXIP1U7CJeuB7SUREdIfGvr/vxpYbfZJbiT8laLkhIiIiEcONPiluh5vaZRiIiIjI6Bhu9EkmB2S3b6m6WtpaiIiI2iiGG32SyfhoioiISGIMN/rGcENERCQpScPN/v37ERUVBS8vL8hkMmzdurXJ1x46dAgKhQK9evUyWH0twnBDREQkKUnDTWlpKUJDQ7Fq1apmXVdQUIDo6GiMGjXKQJU9AIYbIiIiSUm6/MKYMWMwZsyYZl/36quv4tlnn4VcLm9Wa49RKBhuiIiIpGRyfW6++eYbXL16FUuWLJG6lIbVttzUMNwQERFJwaQWzkxMTMSCBQtw4MABKBRNK72yshKVlZXav4uKigxVnqg23GiqAUFTNzTcwIYPH45evXph5cqVRnk/IiKi1spkWm7UajWeffZZLF26FJ07d27ydbGxsVCpVNrN19fXgFUCsFAAkIm/c64bIiIiozOZcFNcXIyTJ09i5syZUCgUUCgUeP/993H27FkoFArs3r27wesWLlyIwsJC7Xbt2jXDFsq5boiIiCRlMuHGwcEB58+fR3x8vHZ79dVXERwcjPj4eISFhTV4nVKphIODg85mcBKuDg4At27dQnR0NJycnGBra4sxY8YgMTFRezwtLQ1RUVFwcnKCnZ0dunfvjh07dmivfe655+Dq6gobGxsEBQXhm2++keRzEBERtYSkfW5KSkqQlJSk/TslJQXx8fFo3749OnTogIULFyIjIwPr1q2DhYUFevTooXO9m5sbrK2t6+3XK0EAqsuaeY0aqC4HKgoBxQOsiG1pK7YENdPkyZORmJiIbdu2wcHBAfPnz8ejjz6KixcvwtLSEjNmzEBVVRX2798POzs7XLx4Efb29gCARYsW4eLFi/j999/h4uKCpKQklJeXt/wzEBERGZmk4ebkyZMYMWKE9u85c+YAAGJiYrB27VpkZmYiPT1dqvJE1WXAh17SvPfbNwAru2ZdUhtqDh06hIEDBwIA1q9fD19fX2zduhVPPfUU0tPTMWHCBPTs2RMA0KlTJ+316enp6N27N/r16wcA8PPz089nISIiMhJJw83w4cMhCMI9j69du7bR69977z289957+i3KxCUkJEChUOg8pnN2dkZwcDASEhIAAG+88QamT5+OnTt3IiIiAhMmTEBISAgAYPr06ZgwYQJOnz6Nhx9+GOPGjdOGJCIiIlNgUkPBJWFpK7agNEdlCZCfDMiVgFuXB3tvA3jppZcQGRmJ7du3Y+fOnYiNjcXy5cvx+uuvY8yYMUhLS8OOHTuwa9cujBo1CjNmzMDHH39skFqIiIj0zWQ6FEtGJhMfDTVns3EELG0AC7kYUJp7fe3Wgv42Xbt2RU1NDY4dO6bdd/PmTVy+fBndunXT7vP19cWrr76Kn3/+GXPnzsW//vUv7TFXV1fExMTgu+++w8qVK/H1118/0C0kIiIyJrbcGELtaCkIgKbmjr8NLygoCGPHjsW0adPw1VdfoV27dliwYAG8vb0xduxYAMCbb76JMWPGoHPnzrh16xb27NmDrl27AgAWL16Mvn37onv37qisrMRvv/2mPUZERGQK2HJjCDILwEK64eDffPMN+vbti8cffxzh4eEQBAE7duyApaVYk1qtxowZM9C1a1c88sgj6Ny5M7744gsAgJWVFRYuXIiQkBAMHToUcrkcGzduNPpnICIiaimZ0FiPXjNUVFQElUqFwsLCenPeVFRUICUlBf7+/rC2foAh3ACQdwWoKgWc/AAbpwd7LROk13tJRERtXmPf33djy42hcAFNIiIiSTDcGAqXYCAiIpIEw42hMNwQERFJguHGUBhuiIiIJMFw0wC99LG+M9y0rT7bAPR0D4mIiFqA4eYOtUOly8qauVBmQ2rnthE04kKabUztPay9p0RERMbCSfzuIJfL4ejoiJycHACAra0tZC2YJVhLLQeEGqCkGLCy0VOVrZsgCCgrK0NOTg4cHR0hl8ulLomIiNoYhpu7eHh4AIA24DyQ4pviY6kCiMsxtCGOjo7ae0lERGRMDDd3kclk8PT0hJubG6qrqx/sxX5fDSTHAYPnAb0m6adAE2BpackWGyIikgzDzT3I5fIH/4K2tQdKrgGFiQBn6SUiIjIKdig2JMcO4s+CNGnrICIiakMYbgxJ5Sv+LLgmbR1ERERtCMONITneDjeFDDdERETGwnBjSLUtN2U3xRXCiYiIyOAYbgzJxhFQ3l6WvfC6pKUQERG1FQw3hsZ+N0REREbFcGNo2n436dLWQURE1EYw3BgaW26IiIiMiuHG0DhiioiIyKgYbgyNLTdERERGxXBjaLWzFLPlhoiIyCgYbgyttuWmOBNQP+BCnERERHRfDDeGZucKyJWAoAGKMqSuhoiIyOwx3BiahQWg8hF/Z78bIiIig2O4MQb2uyEiIjIahhtjqB0OXsCJ/IiIiAyN4cYYVLdbbvhYioiIyOAYboyBSzAQEREZDcONMXAiPyIiIqNhuDGG2pabogxAo5G2FiIiIjPHcGMM7bwAmRxQVwEl2VJXQ0REZNYYboxBrgAcvMTfORyciIjIoBhujEXF4eBERETGwHBjLNoRU2y5ISIiMiSGG2PhiCkiIiKjYLgxFrbcEBERGQXDjbGw5YaIiMgoGG6M5c7FMwVB2lqIiIjMGMONsah8xJ9VJUD5LWlrISIiMmMMN8ZiaQPYuYq/s98NERGRwTDcGJMjVwcnIiIyNIYbY+JEfkRERAbHcGNMHA5ORERkcAw3xqSqfSzFlhsiIiJDYbgxJrbcEBERGZyk4Wb//v2IioqCl5cXZDIZtm7d2uj5P//8M0aPHg1XV1c4ODggPDwc//vf/4xTrD5wIj8iIiKDkzTclJaWIjQ0FKtWrWrS+fv378fo0aOxY8cOnDp1CiNGjEBUVBTOnDlj4Er1pLblpjyfc90QEREZiELKNx8zZgzGjBnT5PNXrlyp8/eHH36IX375Bb/++it69+6t5+oMwFoFuHYFchOAy38AvZ6RuiIiIiKzY9J9bjQaDYqLi9G+fft7nlNZWYmioiKdTVLdx4k//9oiaRlERETmyqTDzccff4ySkhI8/fTT9zwnNjYWKpVKu/n6+hqxwgZ0Gyf+TN4NlBdIWQkREZFZMtlw8/3332Pp0qX44Ycf4Obmds/zFi5ciMLCQu127ZrEnXnduoiPpjTVwOUd0tZCRERkhkwy3GzcuBEvvfQSfvjhB0RERDR6rlKphIODg84mue7jxZ9/bZW0DCIiInNkcuFmw4YNmDJlCjZs2IDHHntM6nJaprbfDR9NERER6Z2k4aakpATx8fGIj48HAKSkpCA+Ph7p6eIMvgsXLkR0dLT2/O+//x7R0dFYvnw5wsLCkJWVhaysLBQWFkpRfsu5BgNu3fhoioiIyAAkDTcnT55E7969tcO458yZg969e2Px4sUAgMzMTG3QAYCvv/4aNTU1mDFjBjw9PbXbrFmzJKn/gdR2LOaoKSIiIr2SCYIgSF2EMRUVFUGlUqGwsFDa/je5l4FVAwALS+CtRMDGSbpaiIiIWrnmfH+bXJ8bs3Hno6lLfDRFRESkLww3Uqp9NHVxq5RVEBERmRWGGylpR03t4VpTREREesJwIyU+miIiItI7hhup1U7ox0dTREREesFwIzXtWlN8NEVERKQPDDdSc+0MuHXnoykiIiI9YbhpDWo7FnNCPyIiogfGcNMa1D6auspHU0RERA+K4aY10D6aqgEubZe6GiIiIpPGcNNa1I6a+murpGUQERGZOoab1qK2383VPUBZvqSlEBERmTKGm9bCJaju0dRljpoiIiJqKYab1kT7aIqjpoiIiFqK4aY10T6a2stHU0RERC3EcNOauAQB7j04aoqIiOgBMNy0NrVz3nCtKSIiohZhuGlt7nw0VZonZSVEREQmieGmtXEJAjxDxUdTP78MqGukroiIiMikMNy0RlGfAgobIDkO2Pmu1NUQERGZFIab1sirF/DEV+Lvx74ETq2VshoiIiKTwnDTWnUbC4x4R/x9+1wg5YC09RAREZkIhpvWbOhbQI8JYv+bH14A8q9KXREREVGrx3DTmslkwNhVgFcfoPwW8P0koKJQ6qqIiIhaNYab1s7SBpj0PdDOC8i7DGyeyhFUREREjWC4MQUOnsAz34sjqJL+BHYtkroiIiKiVovhxlR49QbGfyn+fvQL4NS30tZDRETUSjHcmJLu44HhC8Xft88BUg9KWw8REVErxHBjaobNF0OOpgbY9AJw7YTUFREREbUqDDemRiYDxn4hPqYqzwf+EwH8/ApQlCl1ZURERK0Cw40psrIFnvsJ6PWc+Pe5jcBnfYH9HwPVFdLWRkREJDGGG1Nl5wyM+wKYthvw6Q9UlwK7PwBW9QcubgMEQeoKiYiIJMFwY+q8+wIv7gKe+Jc4F05Bujib8bdRQNYFqasjIiIyOoYbcyCTASFPA6+fBIb+DVBYA6kHgK+GAL/NBopuSF0hERGR0cgEoW09vygqKoJKpUJhYSEcHBykLscwbqUBuxYDF7eKf8ssgKCHgT7R4k+5paTlERERNVdzvr8ZbsxZ6kFg9z+A9MN1++zdxY7IvZ8HnAOkq42IiKgZGG4a0abCTa3cK8CZdUD8BqAsr26/3xCgTwzQNQqwtJauPiIiovtguGlEmww3tWqqgCu/A6fXAUlxAG7/T2/tKE4M2DVKDDwKKymrJCIiqofhphFtOtzcqeAaEL8eOP1foOh63X6lCgh+BOjyOBA4CrCyk65GIiKi2xhuGsFwcxeNGkjZB1z8Bbi0AyjNqTumsBEDTtcooHMkYOMkXZ1ERNSmMdw0guGmERo1cO04cOk3IGGbOGdOLQsF0HEgEDgaCIwA3LqKQ9CJiIiMgOGmEQw3TSQIQNZ5IOFXMezkXNQ93s5LbNUJjAA6DQdsHKWokoiI2giGm0Yw3LTQzWQgcReQ9Kc4QWDNHWtYyeTiEhCBEUDASMAzhHPpEBGRXjHcNILhRg+qy4G0w+KIq6Q/gbzLusctbQGffoDvQ0CHMMBnAGDNe01ERC3HcNMIhhsDKEivCzqpB4CKQt3jMgvArTvQ4aG6TeUjTa1ERGSSGG4awXBjYBqN2JKTfgRIPwZcOwrcSq1/nnOg+AgrYBTgNxhQ2hu9VCIiMh0MN41guJFAcRaQflTcrh0FMs8BgrruuIUl4BsGBIwQOyl7hAIWXNOViIjqMNw0guGmFagoBFIOAMlx4uOsgjTd4zbtxaATMBLoNAJQeUtTJxERtRoMN41guGmF8q8CybuBpN1Ayn6gqlj3uGuX24+wRopz7XDWZCKiNofhphEMN62cuhq4flJs1UneA9w4DQiauuNyK7FDcm3Yce/JR1hERG1Ac76/Jf1W2L9/P6KiouDl5QWZTIatW7fe95q9e/eiT58+UCqVCAwMxNq1aw1eJxmR3BLoGA6MfBeYFge8lQw89a24ernKF1BXia07f74HfDUU+GcnYMOzwOHPgIxTgLpG6k9AREQSU0j55qWlpQgNDcXUqVPxxBNP3Pf8lJQUPPbYY3j11Vexfv16xMXF4aWXXoKnpyciIyONUDEZnW17oPs4cRMEcTLB5N3ilnoAKL8FXN4ubgBgaQf4DgA6DhJDkndfwNJGyk9ARERG1moeS8lkMmzZsgXjxo275znz58/H9u3bceHCBe2+SZMmoaCgAH/88UeT3oePpcyIuloceZV2SJxUMP1w/Tl25FaAdz+g0zDAf5g4uSBnTyYiMjnN+f6WtOWmuY4cOYKIiAidfZGRkXjzzTfveU1lZSUqKyu1fxcVFRmqPDI2uSXg01fcBr0hzrGTmyAGndrAU5Ithp70w8DeWMDKXuyU3Gm4GHbcurHPDhGRmWlRuNmzZw9GjBih71ruKysrC+7u7jr73N3dUVRUhPLyctjY1H/8EBsbi6VLlxqrRJKShQXg3l3cBkwTH2PlXxX76FzdK/4szwcSd4obANi5Av5DxaDTaRjg5CflJyAiIj1oUbh55JFH4OPjgylTpiAmJga+vr76rktvFi5ciDlz5mj/LioqatX1kh7JZIBzgLj1myK27GRfuB109oktO6W5wIWfxA0AHDvUhR2/IYCDp6QfgYiImq9F4SYjIwP//e9/8e2332Lp0qUYOXIkXnzxRYwbNw5WVlb6rlHLw8MD2dnZOvuys7Ph4ODQYKsNACiVSiiVSoPVRCbEwkJcsdwzRHyMVVMFXD8hBp2U/eLvBenAme/EDQCcg26HnaFi2LFzlvYzEBHRfT1wh+LTp0/jm2++wYYNGwAAzz77LF588UWEhoY2r5AmdijesWMHzp8/r9337LPPIj8/nx2K6cFVlojLQ6TsF7cb8QDu+tfDM7Rujh3fMEDB4ExEZAxGn8Tvxo0b+Prrr7Fs2TIoFApUVFQgPDwcq1evRvfu3e95XUlJCZKSkgAAvXv3xooVKzBixAi0b98eHTp0wMKFC5GRkYF169YBEIeC9+jRAzNmzMDUqVOxe/duvPHGG9i+fXuTh4Iz3FCTld8SH13Vhp2ci7rHLW3FRT9rw45LZ/FRGBER6Z1Rwk11dTV++eUXrFmzBrt27UK/fv3w4osv4plnnkFubi7effddnD59GhcvXrzna+zdu7fBjskxMTFYu3YtJk+ejNTUVOzdu1fnmtmzZ+PixYvw8fHBokWLMHny5CbXzXBDLVacLfbXSd4NXN0jjsS6k4O3uBaW/1BxRJYj+3YREemLwcPN66+/jg0bNkAQBLzwwgt46aWX0KNHD51zsrKy4OXlBY1Gc49XkQbDDemFIIgtObUTCqYdBmoqdM9RdRBDTseB4qSCzgFs2SEiaiGDh5tRo0bhpZdewhNPPHHPzro1NTU4dOgQhg0b1tyXNyiGGzKI6nIg/Yi4HlbaYeDGGUBQ655j51YXdPyHAq7BDDtERE3EhTMbwXBDRlFZIo6+SjssbtdPAOpK3XNUvkDQaCBwtBh2lPbS1EpEZAIMHm5iY2Ph7u6OqVOn6uxfs2YNcnNzMX/+/Oa+pNEw3JAkaiqBjNPizMmpB8XAc2fYkVsBHcKBoIfFwMPOyUREOgwebvz8/PD9999j4MCBOvuPHTuGSZMmISUlpbkvaTQMN9QqVJWJISdplzhb8q1U3eOqDkDgKCDgdgdlGydJyiQiai0MHm6sra2RkJAAf39/nf1Xr15Ft27dUFFRcY8rpcdwQ61O7WrntUEn9ZBuq47MAvDqLa6H1WmEuOo559chojbG4Atn+vr64tChQ/XCzaFDh+Dl5dWSlyRqu2QywCVQ3B6aDlSViq06ybvFDsp5l4GMU+J2YLk4v07HgWLQCRghLv7JR1hERFotCjfTpk3Dm2++ierqaowcORIAEBcXh7/97W+YO3euXgskanOs7IDOkeIGAIUZ4vw6V/eIP0tzgaQ/xQ0A7D3EkBMwUgw89q5SVU5E1Cq06LGUIAhYsGABPv30U1RVVQEQH1XNnz8fixcv1nuR+sTHUmTSBAHI/ksMOrXDzmvKdc/xCKmbNbnDQ3yERURmwWhDwUtKSpCQkAAbGxsEBQWZxAKVDDdkVqorxPWwaicTzDqve9zSVpxXJ2CE2GeHj7CIyERxnptGMNyQWSvJqVsiInl3/SUi7NyATsNud04eDqh8JCiSiKj5jBJuTp48iR9++AHp6enaR1O1fv7555a8pFEw3FCbcecSEVf3io+wqst0z3EOqgs6foMBG0fj10lE1AQGHy21ceNGREdHIzIyEjt37sTDDz+MK1euIDs7G+PHj29R0USkZzIZ4N5d3Aa+Lk4keP3E7c7Je8XRVzcTxe3Ev8Qh59596zom+/QD5JZSfwoiomZrUctNSEgIXnnlFcyYMQPt2rXD2bNn4e/vj1deeQWenp5YunSpIWrVC7bcEN1WXiAOOa8diXUzSfe4VTuxNSdghBh2XILYX4eIJGPwx1J2dnb466+/4OfnB2dnZ+zduxc9e/ZEQkICRo4ciczMzBYXb2gMN0T3UHCtbhTW1b1Aeb7ucQcfMegERoj9djhrMhEZkcEfSzk5OaG4uBgA4O3tjQsXLqBnz54oKChAWVnZfa4molbJ0RfoEy1uGg2Qda4u7KQfAYquA2f+K24yC8Cnvxh0AkYBXr0AC7nUn4CICEALw83QoUOxa9cu9OzZE0899RRmzZqF3bt3Y9euXRg1apS+ayQiY7OwEAOLVy9g8GxxLaz0w0DSbnHywLzLwLVj4rbnH4BNe7GvTuAoMey0c5f6ExBRG9aix1L5+fmoqKiAl5cXNBoNPvroIxw+fBhBQUF499134eTUepur+ViKSA8KrgHJcWLQuboPqCy646BMXP+qy+NA18eB9p0kK5OIzIdB+9zU1NTg+++/R2RkJNzdTe//nTHcEOmZulochZUUJy7+mXlW97hbd6DLY2LQ8Qhhp2QiahGDdyi2tbVFQkICOnbs2OIipcJwQ2RghRnApe3Apd/E0ViCuu6YYwexRafLY4DPAEBhJV2dRGRSDN6heMCAAYiPjzfJcENEBqbyBsJeFreyfODK/8SgkxQHFKQDR78QN0s7wG9Q3SSCXBqCiPSkReHmtddew5w5c3Dt2jX07dsXdnZ2OsdDQkL0UhwRmTjb9kCvZ8StqkycLfnSb0DiLqAsD0jcKW6A7tIQ/sPE0VtERC3QosdSFhYW9V9IJoMgCJDJZFCr1Q1c1TrwsRRRK6DRADl/1c2W3NDSEO0DAP+hgP8QwG8IYO8mRaVE1EoYvM9NWlpao8db8+MqhhuiVqje0hCndfvqAIBr17qg4zdYbBUiojaDq4I3guGGyARUFAKph4DUA0DKASD7/F0nyACPHoDfUCAoAug4mJ2TicycwcPNunXrGj0eHR3d3Jc0GoYbIhNUehNIOygGndQDQO4l3eNKFRA0WhyFFRgBWPPfbSJzY/Bwc/ckfdXV1SgrK4OVlRVsbW2Rn59/jyulx3BDZAaKs8WQc3UvcOUPoDS37pjcSuyr0+UxIPhRoJ2HZGUSkf5I8lgqMTER06dPx1tvvYXIyEh9vKRBMNwQmRmNGrh+Eri8HUj4DchP1j3u3Q/o/Ii4PATXwCIyWZL1uTl58iSef/55XLp06f4nS4ThhsiMCQKQd0Ucbn5pO5BxSve4taM41DxgpLhxuDmRyTD4JH73fDGFAjdu3NDnSxIRNZ1MBrgGi9uQuUBRJnB5hzi/Tsp+oKIAuLhV3ADAOeh20BkhjsBStpOweCLSlxa13Gzbtk3nb0EQkJmZic8//xy+vr74/fff9VagvrHlhqiNUteILTnJu8Ut4yQgaOqOW1gCHR4SOyQHjgLce3DGZKJWxOCPpe6exE8mk8HV1RUjR47E8uXL4enp2dyXNBqGGyICAJQXiJ2Sa8POrVTd4/YeYsgJHAV0GsF5dYgkxnluGsFwQ0QNupl8e2XzP8XQc+eMyTILwLuv2KoTMFL8nR2TiYyK4aYRDDdEdF/VFUD6ESA5Tgw8ORd1j1urbndMvt2yo/KRpEyitsTg4WbChAkYMGAA5s+fr7P/o48+wokTJ/Djjz829yWNhuGGiJqtMON20PlTnFunolD3uEvnuqDTcRBgZStJmUTmzODhxtXVFbt370bPnj119p8/fx4RERHIzs5u7ksaDcMNET0QdQ1w40xdq87dHZPlSnENrM6PAMFj2KpDpCcGHwpeUlICK6v667hYWlqiqKioJS9JRGQa5ArAt7+4DV8AlN8Sh5knxYkdkwuviS08SX8CO+YB7j3FkBP8CODZG7hrQAYR6V+Lwk3Pnj2xadMmLF68WGf/xo0b0a1bN70URkRkEmycgG5jxU0QgNzLwJXfgct/ANePi4t+Zp8H9n8E2LsDnSOBzmPEJSKU9lJXT2SWWhRuFi1ahCeeeALJyckYOXIkACAuLg4bNmxo1f1tiIgMSiYD3LqI2+DZ4oKfiTvFsJO0GyjJBk6vEzcLS8B3gDjMPGAE4NlLbBUiogfW4tFS27dvx4cffoj4+HjY2NggJCQES5YswbBhw/Rdo16xzw0RSaKmEkg7BFz+XVzssyBd97hSJfbVqV0eon0nTiJIdAcOBW8Eww0RSU4QgFspQPIecfRVyr76I7BUvmKLTtDDgP8wwJr/vaK2zeDh5sSJE9BoNAgLC9PZf+zYMcjlcvTr16+5L2k0DDdE1Opo1MCNeODq7bCTfhTQVNcdt1AAHcKBoNFA4GjArStbdajNMXi4GTBgAP72t7/hySef1Nn/888/4//+7/9w7Nix5r6k0TDcEFGrV1UKpB0WR1wl7gLyk3WPO/iIc+oEjRYfY3HBT2oDDB5u7O3tce7cOXTq1Elnf0pKCkJCQlBcXNzclzQahhsiMjk3k+uCTuoBoKai7piFpbiieedHxJFY7f2lq5PIgAw+z41SqUR2dna9cJOZmQmFgr39iYj0yjlA3MJeAarLgdSDYtBJ2gXkX739OGsP8Md8wCX49nDzRwDfMI7AojapRS03zzzzDDIzM/HLL79ApVIBAAoKCjBu3Di4ubnhhx9+0Huh+sKWGyIyK3lJ4uirK3+I62FpauqOWTuKi312jhRHYNm5SFYm0YMy+GOpjIwMDB06FDdv3kTv3r0BAPHx8XB3d8euXbvg6+vbssqNgOGGiMxWeYG4LMSV/4nz65Tf0j3uGSqGnICRYquOQilJmUQtYZSh4KWlpVi/fj3Onj2rnefmmWeegaWlZYuKNhaGGyJqEzRq4PoJsUUncReQfUH3uKWt2FenNuy4dOYILGrVjDbPzcWLF5Geno6qqiqd/f/v//2/lr6kwTHcEFGbVJwtDjNP3i1upTm6xx28xSUh/AaLm2NHhh1qVQwebq5evYrx48fj/PnzkMlkEAQBsjv+JVCr1c2v2kgYboiozRMEIPuvuqCTdhhQV+qeo/KtCzoMO9QKNOf7u0XL086aNQv+/v7IycmBra0tLly4gH379qFfv37Yu3dvs15r1apV8PPzg7W1NcLCwnD8+PFGz1+5ciWCg4NhY2MDX19fzJ49GxUVFY1eQ0REd5DJAI8ewKA3gOitwII04IUtwJC5Yl8cC4W4uvnZDcAvM4BPQoGVPYEtrwLx3wNFmVJ/AqJGtajlxsXFBbt370ZISAhUKhWOHz+O4OBg7N69G3PnzsWZM2ea9DqbNm1CdHQ0Vq9ejbCwMKxcuRI//vgjLl++DDc3t3rnf//995g6dSrWrFmDgQMH4sqVK5g8eTImTZqEFStWNOk92XJDRHQfVaXAtWPikPPUg0DGKd1RWADg2kVc9LPTcMBvECcSJIMz+GMpJycnnD59Gv7+/ggICMC///1vjBgxAsnJyejZsyfKysqa9DphYWHo378/Pv/8cwCARqOBr68vXn/9dSxYsKDe+TNnzkRCQgLi4uK0++bOnYtjx47h4MGDTXpPhhsiomaqDTspB8R+OzfOALjjq8NCAfgMuL3o5wjAqw/n1yG9M/gkfj169MDZs2fh7++PsLAwfPTRR7CyssLXX39db2K/e6mqqsKpU6ewcOFC7T4LCwtERETgyJEjDV4zcOBAfPfddzh+/DgGDBiAq1evYseOHXjhhRda8jGIiKgprOzqRlVhCVCWL86UnHx78sBbqUD6YXHb++Ht+XVGAUGR4jw7ds4SfwBqa1oUbt59912UlpYCAN5//308/vjjGDJkCJydnbFp06YmvUZeXh7UajXc3d119ru7u+PSpUsNXvPss88iLy8PgwcPhiAIqKmpwauvvoq33377nu9TWVmJysq6jnJFRUVNqo+IiO7Btj3Qbay4AUB+itiic3UPcHUfUFEAXPhJ3GQWgHc/oPPDYtjx6MmOyWRwLQo3kZGR2t8DAwNx6dIl5Ofnw8nJSWfUlL7t3bsXH374Ib744guEhYUhKSkJs2bNwgcffIBFixY1eE1sbCyWLl1qsJqIiNq89v7i1m/K7fl1TgKJ/wOu7ASyzwPXj4vb7r8D7bzEBT87R4pDz9lXhwzggea5eRBVVVWwtbXF5s2bMW7cOO3+mJgYFBQU4Jdffql3zZAhQ/DQQw/hn//8p3bfd999h5dffhklJSWwsKg/+KuhlhtfX1/2uSEiMobCDHG25MSdYutO9R19Mi0UgO9D4iOswFGAe0+ggf+OEwFG6HOjD1ZWVujbty/i4uK04Uaj0SAuLg4zZ85s8JqysrJ6AUYulwMA7pXRlEollEpOMU5EJAmVt9ii028KUF0BpB0UW3RqF/1MOyhucUsBOzcx5ASMEjsmcy0saiFJu7PPmTMHMTEx6NevHwYMGICVK1eitLQUU6ZMAQBER0fD29sbsbGxAICoqCisWLECvXv31j6WWrRoEaKiorQhh4iIWilLa7GDcWCE+Hf+VSApTpxI8Oo+cdbksxvEDTLAq9ftSQSHAB0eAqxVUlZPJkTScDNx4kTk5uZi8eLFyMrKQq9evfDHH39oOxmnp6frtNS8++67kMlkePfdd5GRkQFXV1dERUXhH//4h1QfgYiIWqp9J2BAJ2DANKCmCrh2VAw7SXFiX50bZ8Tt8Gdix2SPkLoZkzuEAzaOUn8CaqUk63MjFc5zQ0RkAoqzxNactINA6iEgP/muE27Psuw3BOg4COg4UBzFRWbLaAtnmiKGGyIiE1SUCaQdEufXST0E3Ey86wQZ4N5dbNXpOEjcOL+OWWG4aQTDDRGRGSjOqlseIu0QkHel/jlu3erCjt8Qhh0Tx3DTCIYbIiIzVJwthpy0Q2Lgyb17MlgZ4BkqjsLqNELsoKzgSFpTwnDTCIYbIqI2oCS3LuikHgRyE3SPK2zEfjq1Yce9O2dObuUYbhrBcENE1AYVZd5eImKvuExESbbucTu3uoU/Ow0HHLyMXyM1iuGmEQw3RERtnCAAOQliyEneI7bw3DlzMgC4BNe16vgN4jIRrQDDTSMYboiISEdNJXDteF3YuXEGwB1fjRYKwGeA2KLjP1Tsu2NlK1W1bRbDTSMYboiIqFFl+UDK/rpHWLdSdY/L5OJILO8+t7e+gGtXQC7pvLhmj+GmEQw3RETULPkpda06147V768DiB2UPUPFoOPTF+g4GGjnbvxazRjDTSMYboiIqMUEASi6AWScAm6cvv0zHqgsqn+ua1fxMVanYeJcO1wu4oEw3DSC4YaIiPRKowFuJtWFnfSjQNZ56PTbkVkAnr3EsOM/VFwbi/12moXhphEMN0REZHBl+eJSESn7xTWy7l4uwsIS8B0A+A8Tw45PP0BuKU2tJoLhphEMN0REZHRFN8SgUxt2iq7rHre0EycVrH2M5d4TsLCQptZWiuGmEQw3REQkKUEA8q8CKfvEoJN6ACi7qXuOjZO4HlbtYyyXzm1+BmWGm0Yw3BARUaui0QA5f4lBJ2UfkHYYqCrRPcfOTVwE1H8I4DcUcA5oc2GH4aYRDDdERNSqqauBjNNi0Ek9IE4wWFOhe047T7Flx2+wuAiocyBgIZemXiNhuGkEww0REZmU6gog4ySQckAMO9dPAOoq3XMU1oBbV8C9B+ARAnj0EBcDtVZJU7MBMNw0guGGiIhMWnW52JqTekAMPFnn6q+NVcuxg9g52auX2HfHu6/JjspiuGkEww0REZkVjVqcRTn7PJB1Aci+IP68e0QWAFjZ143K8h8mtvSYyKgshptGMNwQEVGbUJYPZP8lhp30o+Iw9PJ83XNsne8YlTWsVXdUZrhpBMMNERHdKf1mGb7clwSVjRVeGdoJTnZWUpdkGBqNGHRS9t97VJatC+DTX5xU0HcA4NUHUNpLU+9dGG4awXBDREQAcKu0Cp/tTsJ/j6aiWi1+FbazVmDmiEDEDPSDtaV5jz4SR2WdqptY8Prx+h2VZRZix2SfAWLo8R0AtO8kSesOw00jGG6IiNq2imo11h5Oxao9SSiuqAEADAp0xs2SKlzKKgYAeDvaYO7DnTGulzcsLFrnY5qmyi+tQllVDbwdbSBrLJTUVAKZ58TRWNePA9dONNxvx85NHH7eIVz86RECyBWG+wC3Mdw0guGGiKht0mgEbI3PwPKdV5BRUA4A6OLRDm8/2hVDO7tCrRGw5UwGlu+8jMxCcV6Z7l4OWDimKwYHuUhZerNU1WhwKu0W9ifm4kBiLi5kiCuW+7a3wZAgVwwNckF4gAtUNk0YNVV0QxyZdf2E+DMzvn7rjqUd4Nsf6DBQDDs+/QArO71/LoabRjDcEBG1PQcT8/DhjgRczBS/6D1V1pj7cDDG9/aG/K6WmYpqNdYcSsGXe5JRXCm27Azr7IoFY7qgq2fr+94QBAHJuaU4kJiLA4l5OHr1Jsqq1DrnKCxkqNHUfd3LLWTo5euIIUEuGBLkilAfFRTyJoyaqq4QA07aYbGT8rWjQEWh7jkWCrGD8gtb9PDp6jDcNILhRhoajYCLmUU4kJiH8xkFUGva1D92RCShvJIqnEq7BQBop1Rg+ogATB3kf98+NfmlVfg0LhHfHU1DjUaATAYMDnSBrVXr6YujEYCLN4q0LVG1XOyVGBrkgiGdXTA40BV2SjmOXc3Hvitia05ybqnO+Q7WCvTp6ASlonnDwmWCBp5VqQiqOI/AigsILD+P9upcJDsORMCbvz/w57sTw00jGG6MJ6e4Ageu5OFAYi4OJuUhr6Tq/hcRERmApVyG5x/qiNdHBqF9M0dDpeaV4p//u4zt5zMNVN2Ds5JboL+/E4YGuWJIkCu6eLRrtK/Q9VtlOJiYhwOJeTiYlIfC8mq91eKNXPTzssInbzyrt9cEGG4axXBjODVqDY5ezcf+xFzsv5Kr7ZhXy9ZKjvBOzniokzNsla3n//kQkXmzkMkwKMAFHZxtH+h1LmQU4uz1Av0UpUfejjYI83eGTQtblNQaAeeuFyAhsxgC9BMJXOyViOzuoZfXqsVw0wiGG8MQBAEvfnsSuy/laPfJZEBPb5X2mW6fDk6wamaTJxEREdC872/Dj92iNuGX+BvYfSkHVgoLjA31wpDOrhgU4Axne6XUpRERURvDcEMPrKiiGn/fngAAmDUqCDNGBEpcERERtWV8RkAPbMXOK8grqUQnFzu8NMRf6nKIiKiNY7ihB3IhoxDrjqQCAN4f2wNKBTsKExGRtBhuqMU0GgHvbr0AjQA8HuJpUjN4EhGR+WK4oRb74eQ1xF8rgL1SgUWPd5O6HCIiIgAMN9RC+aVVWPbHJQDAmxFBcHewlrgiIiIiEcMNtchHf1xCQVk1uni0w+SBflKXQ0REpMVwQ812Ov0WNp64BgD4YFyPpi22RkREZCT8VqJmqVFr8O6WCwCAJ/v6oL9fe4krIiIi0sVwQ83y3dE0XMwsgsrGEgvHdJG6HCIionoYbqjJcooqsHznFQDAW5HBXFqBiIhaJYYbarIPdySguLIGoT4qPDOgg9TlEBERNYjhhprkcHIetsbfgEwmdiKWW8ikLomIiKhBDDd0X4Ig4P1fLwIAngvrgBAfR2kLIiIiagTDDd1XYXk1LmUVAwDmjg6WuBoiIqLGMdzQfd0oqAAAtLezgpOdlcTVEBERNY7hhu4rs7AcAOCp4hILRETU+jHc0H3dKBRbbjxVNhJXQkREdH8MN3RfmQViy42XI1tuiIio9WO4ofvKZMsNERGZEMnDzapVq+Dn5wdra2uEhYXh+PHjjZ5fUFCAGTNmwNPTE0qlEp07d8aOHTuMVG3blMGWGyIiMiEKKd9806ZNmDNnDlavXo2wsDCsXLkSkZGRuHz5Mtzc3OqdX1VVhdGjR8PNzQ2bN2+Gt7c30tLS4OjoaPzi25C6DsVsuSEiotZP0nCzYsUKTJs2DVOmTAEArF69Gtu3b8eaNWuwYMGCeuevWbMG+fn5OHz4MCwtLQEAfn5+xiy5zdFoBGTdfizFlhsiIjIFkj2WqqqqwqlTpxAREVFXjIUFIiIicOTIkQav2bZtG8LDwzFjxgy4u7ujR48e+PDDD6FWq+/5PpWVlSgqKtLZqOnySitRrRYgkwHuDgw3RETU+kkWbvLy8qBWq+Hu7q6z393dHVlZWQ1ec/XqVWzevBlqtRo7duzAokWLsHz5cvz973+/5/vExsZCpVJpN19fX71+DnOXeXsCP7d2SljKJe+iRUREdF8m9W2l0Wjg5uaGr7/+Gn379sXEiRPxzjvvYPXq1fe8ZuHChSgsLNRu165dM2LFpo/9bYiIyNRI1ufGxcUFcrkc2dnZOvuzs7Ph4eHR4DWenp6wtLSEXC7X7uvatSuysrJQVVUFK6v6SwMolUoolUr9Ft+G1C69wP42RERkKiRrubGyskLfvn0RFxen3afRaBAXF4fw8PAGrxk0aBCSkpKg0Wi0+65cuQJPT88Ggw09uBsFbLkhIiLTIuljqTlz5uBf//oXvv32WyQkJGD69OkoLS3Vjp6Kjo7GwoULtedPnz4d+fn5mDVrFq5cuYLt27fjww8/xIwZM6T6CGavbgI/ttwQEZFpkHQo+MSJE5Gbm4vFixcjKysLvXr1wh9//KHtZJyeng4Li7r85evri//973+YPXs2QkJC4O3tjVmzZmH+/PlSfQSzd+N2nxtvR7bcEBGRaZAJgiBIXYQxFRUVQaVSobCwEA4ODlKX0+o99GEcsooqsHXGIPTydZS6HCIiaqOa8/1tUqOlyLhq1BrkFN/uUMzHUkREZCIYbuiesosroREAS7kMLvYccUZERKaB4YbuKfP2SCl3B2tYWMgkroaIiKhpGG7onrSrgXMYOBERmRCGG7on7TBwTuBHREQmhOGG7qn2sZQXh4ETEZEJYbihe7pRyJFSRERkehhu6J64aCYREZkihhu6p8wC9rkhIiLTw3BDDaqoVuNmaRUAjpYiIiLTwnBDDaodKWVtaQFHW0uJqyEiImo6hhtqUOYdc9zIZJzAj4iITAfDDTVIO1KKw8CJiMjEMNxQg2pbbjw5DJyIiEwMww016IZ2dmK23BARkWlhuKEG1c5xwwn8iIjI1DDcUIPq5rhhyw0REZkWhhtq0I0CttwQEZFpYriheoorqlFcWQOALTdERGR6GG6ontoJ/BysFbBXKiSuhoiIqHkYbqge7SMpttoQEZEJYrihempbbjjHDRERmSKGG6pHO4EfW26IiMgEMdxQPdqlF9hyQ0REJojhhuq5oV16gS03RERkehhuqB5tnxtHttwQEZHpYbghHYIgaFtuvNnnhoiITBDDDem4VVaNyhoNAMCDfW6IiMgEMdyQjtpWGxd7KygVcomrISIiaj6GG9JRN8cNH0kREZFpYrghHZmFtSOl+EiKiIhME8MN6cjg0gtERGTiGG5IR2YBl14gIiLTxnBDOmofS7HlhoiITBXDDem4cbvlxosT+BERkYliuCEttUZAdhFHSxERkWljuCGtvJJK1GgEWMgAt3ZKqcshIiJqEYYb0qqdwM/dwRoKOf/RICIi08RvMNK6wZFSRERkBhhuSEs7gR9HShERkQljuCGt2pYbrgZORESmjOGGtLj0AhERmQOGG9K6wUUziYjIDDDckFamdl0pttwQEZHpYrghAEBVjQa5JZUA2HJDRESmjeGGAADZRRUQBMBKbgFnOyupyyEiImoxhhsCUDeBn4fKGhYWMomrISIiajmGGwIAZBZywUwiIjIPDDcEALhxexi4F/vbEBGRiWsV4WbVqlXw8/ODtbU1wsLCcPz48SZdt3HjRshkMowbN86wBbYBmbVLL7DlhoiITJzk4WbTpk2YM2cOlixZgtOnTyM0NBSRkZHIyclp9LrU1FTMmzcPQ4YMMVKl5q1uAj+23BARkWmTPNysWLEC06ZNw5QpU9CtWzesXr0atra2WLNmzT2vUavVeO6557B06VJ06tTJiNWar9qlF9jnhoiITJ2k4aaqqgqnTp1CRESEdp+FhQUiIiJw5MiRe173/vvvw83NDS+++KIxymwTbrDlhoiIzIRCyjfPy8uDWq2Gu7u7zn53d3dcunSpwWsOHjyI//znP4iPj2/Se1RWVqKyslL7d1FRUYvrNVflVWoUlFUDYIdiIiIyfZI/lmqO4uJivPDCC/jXv/4FFxeXJl0TGxsLlUql3Xx9fQ1cpempbbWxs5LDwUbSvEtERPTAJP0mc3FxgVwuR3Z2ts7+7OxseHh41Ds/OTkZqampiIqK0u7TaDQAAIVCgcuXLyMgIEDnmoULF2LOnDnav4uKihhw7lI3UsoGMhkn8CMiItMmabixsrJC3759ERcXpx3OrdFoEBcXh5kzZ9Y7v0uXLjh//rzOvnfffRfFxcX45JNPGgwtSqUSSqXSIPWbi7r+NuxMTEREpk/yZxBz5sxBTEwM+vXrhwEDBmDlypUoLS3FlClTAADR0dHw9vZGbGwsrK2t0aNHD53rHR0dAaDefmq62pYb9rchIiJzIHm4mThxInJzc7F48WJkZWWhV69e+OOPP7SdjNPT02FhYVJdg0yOdo4bDgMnIiIzIBMEQZC6CGMqKiqCSqVCYWEhHBwcpC6nVXjhP8dwIDEPH00IwdP92R+JiIhan+Z8f7NJhLSLZrLlhoiIzAHDTRsnCAIyC24vmunIPjdERGT6GG7auKKKGpRWqQGwQzEREZkHyTsUk3Ryiyvx8f8uAwAcbS1hYyWXuCIiIqIHx3DTBpVV1eBf+1Pw9f5kbavNE719JK6KiIhIPxhu2pAatQY/nrqOFbuuILdYXG8r1EeFhY92xUOdnCWujoiISD8YbtoAQRCw+1IOlv1+CYk5JQAA3/Y2+FtkFzwe4sklF4iIyKww3OhJZY1a2xrSmmQWVmD5zss4ejUfgNi35vWRQXj+oQ5QKtjHhoiIzA/DjZ78daMIT3xxWOoy7slKYYEpg/zw2vBAqGwspS6HiIjIYBhu9EQGQKlofSPrFRYyRHb3wNzIYHhzHhsiImoDGG70pHcHJ1z++xipyyAiImrzWl9TAxEREdEDYLghIiIis8JwQ0RERGaF4YaIiIjMCsMNERERmRWGGyIiIjIrDDdERERkVhhuiIiIyKww3BAREZFZYbghIiIis8JwQ0RERGaF4YaIiIjMCsMNERERmRWGGyIiIjIrCqkLMDZBEAAARUVFEldCRERETVX7vV37Pd6YNhduiouLAQC+vr4SV0JERETNVVxcDJVK1eg5MqEpEciMaDQa3LhxA+3atYNMJtPraxcVFcHX1xfXrl2Dg4ODXl+b6uP9Ni7eb+Pi/TYu3m/jasn9FgQBxcXF8PLygoVF471q2lzLjYWFBXx8fAz6Hg4ODvyXw4h4v42L99u4eL+Ni/fbuJp7v+/XYlOLHYqJiIjIrDDcEBERkVlhuNEjpVKJJUuWQKlUSl1Km8D7bVy838bF+21cvN/GZej73eY6FBMREZF5Y8sNERERmRWGGyIiIjIrDDdERERkVhhuiIiIyKww3OjJqlWr4OfnB2tra4SFheH48eNSl2Q29u/fj6ioKHh5eUEmk2Hr1q06xwVBwOLFi+Hp6QkbGxtEREQgMTFRmmJNXGxsLPr374927drBzc0N48aNw+XLl3XOqaiowIwZM+Ds7Ax7e3tMmDAB2dnZElVs2r788kuEhIRoJzILDw/H77//rj3Oe21Yy5Ytg0wmw5tvvqndx3uuP++99x5kMpnO1qVLF+1xQ95rhhs92LRpE+bMmYMlS5bg9OnTCA0NRWRkJHJycqQuzSyUlpYiNDQUq1atavD4Rx99hE8//RSrV6/GsWPHYGdnh8jISFRUVBi5UtO3b98+zJgxA0ePHsWuXbtQXV2Nhx9+GKWlpdpzZs+ejV9//RU//vgj9u3bhxs3buCJJ56QsGrT5ePjg2XLluHUqVM4efIkRo4cibFjx+Kvv/4CwHttSCdOnMBXX32FkJAQnf285/rVvXt3ZGZmareDBw9qjxn0Xgv0wAYMGCDMmDFD+7darRa8vLyE2NhYCasyTwCELVu2aP/WaDSCh4eH8M9//lO7r6CgQFAqlcKGDRskqNC85OTkCACEffv2CYIg3ltLS0vhxx9/1J6TkJAgABCOHDkiVZlmxcnJSfj3v//Ne21AxcXFQlBQkLBr1y5h2LBhwqxZswRB4D/f+rZkyRIhNDS0wWOGvtdsuXlAVVVVOHXqFCIiIrT7LCwsEBERgSNHjkhYWduQkpKCrKwsnfuvUqkQFhbG+68HhYWFAID27dsDAE6dOoXq6mqd+92lSxd06NCB9/sBqdVqbNy4EaWlpQgPD+e9NqAZM2bgscce07m3AP/5NoTExER4eXmhU6dOeO6555Ceng7A8Pe6zS2cqW95eXlQq9Vwd3fX2e/u7o5Lly5JVFXbkZWVBQAN3v/aY9QyGo0Gb775JgYNGoQePXoAEO+3lZUVHB0ddc7l/W658+fPIzw8HBUVFbC3t8eWLVvQrVs3xMfH814bwMaNG3H69GmcOHGi3jH+861fYWFhWLt2LYKDg5GZmYmlS5diyJAhuHDhgsHvNcMNETVoxowZuHDhgs4zctK/4OBgxMfHo7CwEJs3b0ZMTAz27dsndVlm6dq1a5g1axZ27doFa2trqcsxe2PGjNH+HhISgrCwMHTs2BE//PADbGxsDPrefCz1gFxcXCCXy+v18M7OzoaHh4dEVbUdtfeY91+/Zs6cid9++w179uyBj4+Pdr+HhweqqqpQUFCgcz7vd8tZWVkhMDAQffv2RWxsLEJDQ/HJJ5/wXhvAqVOnkJOTgz59+kChUEChUGDfvn349NNPoVAo4O7uzntuQI6OjujcuTOSkpIM/s83w80DsrKyQt++fREXF6fdp9FoEBcXh/DwcAkraxv8/f3h4eGhc/+Liopw7Ngx3v8WEAQBM2fOxJYtW7B79274+/vrHO/bty8sLS117vfly5eRnp7O+60nGo0GlZWVvNcGMGrUKJw/fx7x8fHarV+/fnjuuee0v/OeG05JSQmSk5Ph6elp+H++H7hLMgkbN24UlEqlsHbtWuHixYvCyy+/LDg6OgpZWVlSl2YWiouLhTNnzghnzpwRAAgrVqwQzpw5I6SlpQmCIAjLli0THB0dhV9++UU4d+6cMHbsWMHf318oLy+XuHLTM336dEGlUgl79+4VMjMztVtZWZn2nFdffVXo0KGDsHv3buHkyZNCeHi4EB4eLmHVpmvBggXCvn37hJSUFOHcuXPCggULBJlMJuzcuVMQBN5rY7hztJQg8J7r09y5c4W9e/cKKSkpwqFDh4SIiAjBxcVFyMnJEQTBsPea4UZPPvvsM6FDhw6ClZWVMGDAAOHo0aNSl2Q29uzZIwCot8XExAiCIA4HX7RokeDu7i4olUph1KhRwuXLl6Ut2kQ1dJ8BCN988432nPLycuG1114TnJycBFtbW2H8+PFCZmamdEWbsKlTpwodO3YUrKysBFdXV2HUqFHaYCMIvNfGcHe44T3Xn4kTJwqenp6ClZWV4O3tLUycOFFISkrSHjfkvZYJgiA8ePsPERERUevAPjdERERkVhhuiIiIyKww3BAREZFZYbghIiIis8JwQ0RERGaF4YaIiIjMCsMNERERmRWGGyJq8/bu3QuZTFZvnRsiMk0MN0RERGRWGG6IiIjIrDDcEJHkNBoNYmNj4e/vDxsbG4SGhmLz5s0A6h4Zbd++HSEhIbC2tsZDDz2ECxcu6LzGTz/9hO7du0OpVMLPzw/Lly/XOV5ZWYn58+fD19cXSqUSgYGB+M9//qNzzqlTp9CvXz/Y2tpi4MCBuHz5smE/OBEZBMMNEUkuNjYW69atw+rVq/HXX39h9uzZeP7557Fv3z7tOW+99RaWL1+OEydOwNXVFVFRUaiurgYghpKnn34akyZNwvnz5/Hee+9h0aJFWLt2rfb66OhobNiwAZ9++ikSEhLw1Vdfwd7eXqeOd955B8uXL8fJkyehUCgwdepUo3x+ItIvLpxJRJKqrKxE+/bt8eeffyI8PFy7/6WXXkJZWRlefvlljBgxAhs3bsTEiRMBAPn5+fDx8cHatWvx9NNP47nnnkNubi527typvf5vf/sbtm/fjr/++gtXrlxBcHAwdu3ahYiIiHo17N27FyNGjMCff/6JUaNGAQB27NiBxx57DOXl5bC2tjbwXSAifWLLDRFJKikpCWVlZRg9ejTs7e2127p165CcnKw9787g0759ewQHByMhIQEAkJCQgEGDBum87qBBg5CYmAi1Wo34+HjI5XIMGzas0VpCQkK0v3t6egIAcnJyHvgzEpFxKaQugIjatpKSEgDA9u3b4e3trXNMqVTqBJyWsrGxadJ5lpaW2t9lMhkAsT8QEZkWttwQkaS6desGpVKJ9PR0BAYG6my+vr7a844ePar9/datW7hy5Qq6du0KAOjatSsOHTqk87qHDh1C586dIZfL0bNnT2g0Gp0+PERkvthyQ0SSateuHebNm4fZs2dDo9Fg8ODBKCwsxKFDh+Dg4ICOHTsCAN5//304OzvD3d0d77zzDlxcXDBu3DgAwNy5c9G/f3988MEHmDhxIo4cOYLPP/8cX3zxBQDAz88PMTExmDp1Kj799FOEhoYiLS0NOTk5ePrpp6X66ERkIAw3RCS5Dz74AK6uroiNjcXVq1fh6OiIPn364O2339Y+Flq2bBlmzZqFxMRE9OrVC7/++iusrKwAAH369MEPP/yAxYsX44MPPoCnpyfef/99TJ48WfseX375Jd5++2289tpruHnzJjp06IC3335bio9LRAbG0VJE1KrVjmS6desWHB0dpS6HiEwA+9wQERGRWWG4ISIiIrPCx1JERERkVthyQ0RERGaF4YaIiIjMCsMNERERmRWGGyIiIjIrDDdERERkVhhuiIiIyKww3BAREZFZYbghIiIis8JwQ0RERGbl/wNdQylSYyHAJwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Un modelo de Deep Learning para generar un ChatBot 🤖\n",
    "\n",
    "En esta sección abordaremos un técnica simple para generar modelos de conversación con ChatBot a través de aprendizaje de máquina. Antes de ello conviene traer a discusión el tipo de ChatBots que existen:\n",
    "\n",
    "### 4.2.1 Tipos de ChatBot Conversacionales\n",
    "\n",
    "La creación de ChatBots de conversación tiene una historia larga y esencialmente obecede al problema que se pretende resolver y las limitaciones tecnológicas existentes. En términos muy generales, podemos decir que se dividen en varios tipos:\n",
    "\n",
    "* **Basados en árboles de decisión:** En términos simples, tratan de guiar al usuario con siguiendo un flujo de información que genera respuestas, de forma que el usuario provee datos a un bot (usualmente a través de una interfaz que le permite seleccionar en un menú gráfico) y como resultado el flujo genera un texto predeterminado; requiere la planificación del flujo basándose en un árbol de decisión. Ver por ejemplo: https://www.dashly.io/custom-chatbot/glossary/decision-trees/\n",
    "\n",
    "\n",
    "* **Basado en reconocimiento de palabras clave**: se refiere a una situación donde el bot procesa el texto con técnicas de NLP, reconociendo  palabras clave en los mensajes de los usuarios para determinar su intención y proporcionar una respuesta adecuada. Por ejemplo, si un usuario escribe \"¿Cuál es el horario de atención?\", el chatbot puede reconocer las palabras clave \"horario\" y \"atención\" y responder con la información correspondiente.\n",
    "\n",
    "    Este tipo de chatbot utiliza algoritmos de procesamiento del lenguaje natural para analizar el texto del usuario y determinar las palabras clave relevantes, luego busca en su base de datos de respuestas para proporcionar una respuesta adecuada.\n",
    "\n",
    "* **Basado en modelos contextuales:** usualmente basando en técnicas de NLP y modelos de aprendizaje automático, se refiere a un escenario donde un chatbot puede recordar la información de la conversación anterior y usarla para guiar la conversación actual. Por ejemplo, si alguien está preguntando acerca de un producto en particular, el chatbot puede recordar lo que se discutió previamente sobre el producto y proporcionar información adicional basada en ese contexto. \n",
    "\n",
    "* **Basados en modelos de generación de lenguaje natural:** Esencialmente usan modelos pueden generar texto automáticamente, como respuestas a preguntas, resúmenes, historias, entre otros. En su mayoría se basan en frameworks predictivos de aprendizaje automático en combinación con algun forma de evaluación y refuerzo de que el contenido que generan es adecuado para las preguntas que les hacen llegar los usuarios (dentro de estos se encuentra ChatGPT).\n",
    "\n",
    "En términos prácticos, la idoneidad de cada tipo de ChatBot depende el problema a resolver y en la práctica es común ver enfoques que híbridos, es decir, que mezclan las técnicas de construcción de bots señaladas arriba.\n",
    "\n",
    "### 4.2.2 Entrenando un modelo de Deep Learning para generar un ChatBot de reconocimiento de palabras clave 🤖\n",
    "\n",
    "Una forma sencilla para crear un modelo conversacional es general un modelo de reconocimiento de palabras clave, que una vez reconocidas, puedan ser atendidas acudiendo a una base de datos de respuestas.\n",
    "\n",
    "Desde el punto de vista de aprendizaje de máquina esto se puede pensar como un problema de aprendizaje supervizado de tipo clasificación, en el siguiente sentido:\n",
    "\n",
    "a. Los valores de entrada es la representación numérica de la pregunta del usuario, donde se han codificado los términos clave.\n",
    "\n",
    "b. La variable objetivo no es más que una categoría del tipo de pregunta al que pertenece dicha pregunta.\n",
    "\n",
    "La respuesta a dicha pregunta simplemente se puede contestar una vez que se ha predicho a que categoríua pertenece y mostrando al usuario la respuesta alojada en la correspondiente base de datos de respuestas.\n",
    "\n",
    "En resumen, esto propone un flujo de trabajo para construir un Chatbot:\n",
    "\n",
    "* Procesar un conjunto de documentos para limpiar el texto correspondiete,\n",
    "* Crear una representación numérica de los documentos limpios, incluyen los términos clave,\n",
    "* Entrenar un modelo usando la representación numérica, donde la variable objetivo en la categoría a la que pertenece la pregunta\n",
    "* Una vez entrenado el modelo, se puede generar una respuesta a una nueva pregunta usando el modelo previamente entrenado, asignado el texto a cualquiera de las respuestas presentes en la base de datos para dicha categoría de pregunta que el modelo predijo.\n",
    "\n",
    "Vamos a ejemplificar lo anterior, usando el conjunto de datos **sample.json**.\n",
    "\n",
    "### 4.2.2.1 Conjunto de datos sample.json\n",
    "\n",
    "Este conjunto de datos se trata de una estructura que contiene una serie de scripts de conversación:\n",
    "* **tag:** es una etiqueta que clasifica el tipo de interacción entre el usuario y el Chatbot (por ejemplo, “saludo”, “despedida”, etcétera),\n",
    "* **patterns:** conversaciones de ejemplo que se espera recibir como consultar al chatbot por parte de los usuarios,\n",
    "* **responses:** es una lista de las respuestas en texto que se espera que los usuarios reciban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sample.json') as f:\n",
    "    conversations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'patterns': ['Hola', 'holaa', 'oliiii', 'hello', 'hi'],\n",
      "  'responses': ['Hola :)', 'Hola crayola ;)', 'Hola! Encantado de verte'],\n",
      "  'tag': 'saludo'},\n",
      " {'patterns': ['como te llamas',\n",
      "               'cual es tu nombre',\n",
      "               'quien eres',\n",
      "               'quien eres tu'],\n",
      "  'responses': ['Mi nombre es ChatBot, soy un bot de conversación :)',\n",
      "                'Soy ChatBot, soy un bot de conversación :)',\n",
      "                'Me llamo ChatBot, soy un bot entrenandose para conversar'],\n",
      "  'tag': 'introduccion'},\n",
      " {'patterns': ['que es un bot', 'que es un bot de conversacion'],\n",
      "  'responses': ['Soy un sistema automatizado que usa inteligencia articifial!',\n",
      "                'Es un robot con inteligencia articifial que quiere conversar'],\n",
      "  'tag': 'explicacion'},\n",
      " {'patterns': ['Adios', 'bye', 'ciao', 'nos vemos'],\n",
      "  'responses': ['Adios!', 'Bye, Bye', 'Vuelve pronto'],\n",
      "  'tag': 'despedida'}]\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos el contenido\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(conversations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso:\n",
    "\n",
    "* Los documentos que contienen las preguntas con las que predeciremos corresponden al campo `patterns`,\n",
    "* La categoría a predecir es el contenido del campo `tag`\n",
    "* Mientras que las respuestas que puede recibir el usuario se encuentran en el campo `responses`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como un paso auxiliar creamos un diccionario con las respuestas del bot por categoria y lo salvamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un diccionario con las respuestas del \n",
    "# bot por categoria y lo salvamos\n",
    "category_answers= {}\n",
    "\n",
    "for conversation in conversations:\n",
    "    cat = conversation['tag']\n",
    "    category_answers[cat] = conversation['responses']\n",
    "\n",
    "\n",
    "pickle.dump(category_answers, open('category_answers.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'saludo': ['Hola :)', 'Hola crayola ;)', 'Hola! Encantado de verte'],\n",
       " 'introduccion': ['Mi nombre es ChatBot, soy un bot de conversación :)',\n",
       "  'Soy ChatBot, soy un bot de conversación :)',\n",
       "  'Me llamo ChatBot, soy un bot entrenandose para conversar'],\n",
       " 'explicacion': ['Soy un sistema automatizado que usa inteligencia articifial!',\n",
       "  'Es un robot con inteligencia articifial que quiere conversar'],\n",
       " 'despedida': ['Adios!', 'Bye, Bye', 'Vuelve pronto']}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.2 Procesamiento de documentos de preguntas\n",
    "\n",
    "Para procesar las preguntas realizaremos la eliminación de signos de puntuación, se convertiran a minúsculas y el texto será lematizado, de manera que podamos identificar la información clave presente en cada una.\n",
    "\n",
    "Esto se puede lograr con spaCy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo de lenguaje en español de Spacy\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# lista para extraer preguntas\n",
    "questions = []\n",
    "\n",
    "for script in conversations:\n",
    "\n",
    "    # extraemos preguntas\n",
    "    question = script['patterns']\n",
    "    questions.append(question)\n",
    "\n",
    "# Consolida todas las preguntas \n",
    "documents = list(itertools.chain.from_iterable(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saludo</td>\n",
       "      <td>Hola</td>\n",
       "      <td>[Hola :), Hola crayola ;), Hola! Encantado de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saludo</td>\n",
       "      <td>holaa</td>\n",
       "      <td>[Hola :), Hola crayola ;), Hola! Encantado de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saludo</td>\n",
       "      <td>oliiii</td>\n",
       "      <td>[Hola :), Hola crayola ;), Hola! Encantado de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saludo</td>\n",
       "      <td>hello</td>\n",
       "      <td>[Hola :), Hola crayola ;), Hola! Encantado de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saludo</td>\n",
       "      <td>hi</td>\n",
       "      <td>[Hola :), Hola crayola ;), Hola! Encantado de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>introduccion</td>\n",
       "      <td>como te llamas</td>\n",
       "      <td>[Mi nombre es ChatBot, soy un bot de conversac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>introduccion</td>\n",
       "      <td>cual es tu nombre</td>\n",
       "      <td>[Mi nombre es ChatBot, soy un bot de conversac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>introduccion</td>\n",
       "      <td>quien eres</td>\n",
       "      <td>[Mi nombre es ChatBot, soy un bot de conversac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>introduccion</td>\n",
       "      <td>quien eres tu</td>\n",
       "      <td>[Mi nombre es ChatBot, soy un bot de conversac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explicacion</td>\n",
       "      <td>que es un bot</td>\n",
       "      <td>[Soy un sistema automatizado que usa inteligen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explicacion</td>\n",
       "      <td>que es un bot de conversacion</td>\n",
       "      <td>[Soy un sistema automatizado que usa inteligen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>despedida</td>\n",
       "      <td>Adios</td>\n",
       "      <td>[Adios!, Bye, Bye, Vuelve pronto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>despedida</td>\n",
       "      <td>bye</td>\n",
       "      <td>[Adios!, Bye, Bye, Vuelve pronto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>despedida</td>\n",
       "      <td>ciao</td>\n",
       "      <td>[Adios!, Bye, Bye, Vuelve pronto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>despedida</td>\n",
       "      <td>nos vemos</td>\n",
       "      <td>[Adios!, Bye, Bye, Vuelve pronto]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tag                       patterns   \n",
       "0        saludo                           Hola  \\\n",
       "0        saludo                          holaa   \n",
       "0        saludo                         oliiii   \n",
       "0        saludo                          hello   \n",
       "0        saludo                             hi   \n",
       "1  introduccion                 como te llamas   \n",
       "1  introduccion              cual es tu nombre   \n",
       "1  introduccion                     quien eres   \n",
       "1  introduccion                  quien eres tu   \n",
       "2   explicacion                  que es un bot   \n",
       "2   explicacion  que es un bot de conversacion   \n",
       "3     despedida                          Adios   \n",
       "3     despedida                            bye   \n",
       "3     despedida                           ciao   \n",
       "3     despedida                      nos vemos   \n",
       "\n",
       "                                           responses  \n",
       "0  [Hola :), Hola crayola ;), Hola! Encantado de ...  \n",
       "0  [Hola :), Hola crayola ;), Hola! Encantado de ...  \n",
       "0  [Hola :), Hola crayola ;), Hola! Encantado de ...  \n",
       "0  [Hola :), Hola crayola ;), Hola! Encantado de ...  \n",
       "0  [Hola :), Hola crayola ;), Hola! Encantado de ...  \n",
       "1  [Mi nombre es ChatBot, soy un bot de conversac...  \n",
       "1  [Mi nombre es ChatBot, soy un bot de conversac...  \n",
       "1  [Mi nombre es ChatBot, soy un bot de conversac...  \n",
       "1  [Mi nombre es ChatBot, soy un bot de conversac...  \n",
       "2  [Soy un sistema automatizado que usa inteligen...  \n",
       "2  [Soy un sistema automatizado que usa inteligen...  \n",
       "3                  [Adios!, Bye, Bye, Vuelve pronto]  \n",
       "3                  [Adios!, Bye, Bye, Vuelve pronto]  \n",
       "3                  [Adios!, Bye, Bye, Vuelve pronto]  \n",
       "3                  [Adios!, Bye, Bye, Vuelve pronto]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(conversations).explode(['patterns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola',\n",
       " 'holaa',\n",
       " 'oliiii',\n",
       " 'hello',\n",
       " 'hi',\n",
       " 'como te llamas',\n",
       " 'cual es tu nombre',\n",
       " 'quien eres',\n",
       " 'quien eres tu',\n",
       " 'que es un bot',\n",
       " 'que es un bot de conversacion',\n",
       " 'Adios',\n",
       " 'bye',\n",
       " 'ciao',\n",
       " 'nos vemos']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora limpiaremos los signos de puntuación y lematizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_processed = []\n",
    "\n",
    "for doc in documents:\n",
    "    tokens = nlp(doc)\n",
    "    # remueve signos de puntuacion y lematiza\n",
    "    new_tokens = [t.orth_ for t in tokens if not  t.is_punct]\n",
    "\n",
    "    # pasa a minusculas\n",
    "    new_tokens = [t.lower() for t in new_tokens]\n",
    "\n",
    "    # une los tokens procesados en una string con espacios\n",
    "    question_processed.append(' '.join(new_tokens))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son las preguntas ya preocesadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola',\n",
       " 'holaa',\n",
       " 'oliiii',\n",
       " 'hello',\n",
       " 'hi',\n",
       " 'como te llamas',\n",
       " 'cual es tu nombre',\n",
       " 'quien eres',\n",
       " 'quien eres tu',\n",
       " 'que es un bot',\n",
       " 'que es un bot de conversacion',\n",
       " 'adios',\n",
       " 'bye',\n",
       " 'ciao',\n",
       " 'nos vemos']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como un paso auxilar, también extraremos las categorias que se asocian a cada una de estas preguntas, usando pandas y la funcionalidad `explode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el archivo de script de conversaciones\n",
    "# y lo extiende por el contenido de patterns\n",
    "df_conversation = pd.DataFrame(conversations).explode(\n",
    "    ['patterns']\n",
    "    ).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patterns</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hola</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>holaa</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oliiii</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>como te llamas</td>\n",
       "      <td>introduccion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cual es tu nombre</td>\n",
       "      <td>introduccion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quien eres</td>\n",
       "      <td>introduccion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quien eres tu</td>\n",
       "      <td>introduccion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>que es un bot</td>\n",
       "      <td>explicacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>que es un bot de conversacion</td>\n",
       "      <td>explicacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Adios</td>\n",
       "      <td>despedida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bye</td>\n",
       "      <td>despedida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ciao</td>\n",
       "      <td>despedida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nos vemos</td>\n",
       "      <td>despedida</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         patterns           tag\n",
       "0                            Hola        saludo\n",
       "1                           holaa        saludo\n",
       "2                          oliiii        saludo\n",
       "3                           hello        saludo\n",
       "4                              hi        saludo\n",
       "5                  como te llamas  introduccion\n",
       "6               cual es tu nombre  introduccion\n",
       "7                      quien eres  introduccion\n",
       "8                   quien eres tu  introduccion\n",
       "9                   que es un bot   explicacion\n",
       "10  que es un bot de conversacion   explicacion\n",
       "11                          Adios     despedida\n",
       "12                            bye     despedida\n",
       "13                           ciao     despedida\n",
       "14                      nos vemos     despedida"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conversation[['patterns', 'tag']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataframe, notamos que la columna `tag` tiene la información de que queremos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.3 Representación numérica de documentos de preguntas\n",
    "\n",
    "Crearemos una bolsa de palabras de los términos presentes en los documentos que recientemene limpiamos, usando `CountVectorizer` de Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instancia el transformador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# crea la bolsa de palabras con la lista de documentos\n",
    "X = vectorizer.fit_transform(question_processed)\n",
    "\n",
    "bow = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reutilizarlo posteriormente, salvaremos en formato pickle al transformador `CountVectorizer` que hemos ajustado recientemente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva el transformador que crea la bolsa de palabras\n",
    "# con el texto limpio de las preguntas\n",
    "pickle.dump(\n",
    "    vectorizer,\n",
    "    open('sample_vectorizer_bow.pkl', 'wb')\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado lo podemos visualizar en la tabla inferior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adios</th>\n",
       "      <th>bot</th>\n",
       "      <th>bye</th>\n",
       "      <th>ciao</th>\n",
       "      <th>como</th>\n",
       "      <th>conversacion</th>\n",
       "      <th>cual</th>\n",
       "      <th>de</th>\n",
       "      <th>eres</th>\n",
       "      <th>es</th>\n",
       "      <th>...</th>\n",
       "      <th>llamas</th>\n",
       "      <th>nombre</th>\n",
       "      <th>nos</th>\n",
       "      <th>oliiii</th>\n",
       "      <th>que</th>\n",
       "      <th>quien</th>\n",
       "      <th>te</th>\n",
       "      <th>tu</th>\n",
       "      <th>un</th>\n",
       "      <th>vemos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    adios  bot  bye  ciao  como  conversacion  cual  de  eres  es  ...   \n",
       "3       0    0    0     0     0             0     0   0     0   0  ...  \\\n",
       "1       0    0    0     0     0             0     0   0     0   0  ...   \n",
       "13      0    0    0     1     0             0     0   0     0   0  ...   \n",
       "10      0    1    0     0     0             1     0   1     0   1  ...   \n",
       "7       0    0    0     0     0             0     0   0     1   0  ...   \n",
       "6       0    0    0     0     0             0     1   0     0   1  ...   \n",
       "5       0    0    0     0     1             0     0   0     0   0  ...   \n",
       "0       0    0    0     0     0             0     0   0     0   0  ...   \n",
       "12      0    0    1     0     0             0     0   0     0   0  ...   \n",
       "8       0    0    0     0     0             0     0   0     1   0  ...   \n",
       "\n",
       "    llamas  nombre  nos  oliiii  que  quien  te  tu  un  vemos  \n",
       "3        0       0    0       0    0      0   0   0   0      0  \n",
       "1        0       0    0       0    0      0   0   0   0      0  \n",
       "13       0       0    0       0    0      0   0   0   0      0  \n",
       "10       0       0    0       0    1      0   0   0   1      0  \n",
       "7        0       0    0       0    0      1   0   0   0      0  \n",
       "6        0       1    0       0    0      0   0   1   0      0  \n",
       "5        1       0    0       0    0      0   1   0   0      0  \n",
       "0        0       0    0       0    0      0   0   0   0      0  \n",
       "12       0       0    0       0    0      0   0   0   0      0  \n",
       "8        0       0    0       0    0      1   0   1   0      0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toma una muestra de 10 renglones de la bolsa de palabras:\n",
    "bow.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar nuestro análisis, ahora podemo fusionar tanto la bolsa de palabras junto con las categorias que vamos a predecir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une la bolsa de palabras de cada pregunta con su categoria\n",
    "processed_data = pd.concat(\n",
    "    [bow,\n",
    "     df_conversation[['tag']]\n",
    "     ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 25 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   adios         15 non-null     int64 \n",
      " 1   bot           15 non-null     int64 \n",
      " 2   bye           15 non-null     int64 \n",
      " 3   ciao          15 non-null     int64 \n",
      " 4   como          15 non-null     int64 \n",
      " 5   conversacion  15 non-null     int64 \n",
      " 6   cual          15 non-null     int64 \n",
      " 7   de            15 non-null     int64 \n",
      " 8   eres          15 non-null     int64 \n",
      " 9   es            15 non-null     int64 \n",
      " 10  hello         15 non-null     int64 \n",
      " 11  hi            15 non-null     int64 \n",
      " 12  hola          15 non-null     int64 \n",
      " 13  holaa         15 non-null     int64 \n",
      " 14  llamas        15 non-null     int64 \n",
      " 15  nombre        15 non-null     int64 \n",
      " 16  nos           15 non-null     int64 \n",
      " 17  oliiii        15 non-null     int64 \n",
      " 18  que           15 non-null     int64 \n",
      " 19  quien         15 non-null     int64 \n",
      " 20  te            15 non-null     int64 \n",
      " 21  tu            15 non-null     int64 \n",
      " 22  un            15 non-null     int64 \n",
      " 23  vemos         15 non-null     int64 \n",
      " 24  tag           15 non-null     object\n",
      "dtypes: int64(24), object(1)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "processed_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como un paso adicional reordenaremos el dataframe obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena aleatoriamente el dataframe:\n",
    "processed_data = processed_data.sample(\n",
    "    frac=1,\n",
    "    random_state=123\n",
    ").reset_index(drop=True)\n",
    "\n",
    "#del processed_data['index']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot Encoding de categorías**\n",
    "\n",
    "Como sabemos, para emplear variables categóricas en  modelos de aprendizaje automático necesitamos realizar la codificación de tales categorías.\n",
    "\n",
    "La estrategia que se puede emplear es el One Hot Encoding, que consiste en representar categorías a través de columnas que indican con 0 y 1, si una categoría está presente. La idea es que esta codificación, de una categoría a un vector de tantas dimensiones como categorías existas, sea el output que va a predecir la red neuronal.\n",
    "\n",
    "En pandas, es facil obtener la representación señalada con el método `get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>despedida</th>\n",
       "      <th>explicacion</th>\n",
       "      <th>introduccion</th>\n",
       "      <th>saludo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    despedida  explicacion  introduccion  saludo\n",
       "0           0            0             1       0\n",
       "1           0            1             0       0\n",
       "2           0            0             0       1\n",
       "3           0            0             0       1\n",
       "4           0            0             1       0\n",
       "5           0            1             0       0\n",
       "6           0            0             1       0\n",
       "7           1            0             0       0\n",
       "8           0            0             0       1\n",
       "9           0            0             0       1\n",
       "10          0            0             1       0\n",
       "11          1            0             0       0\n",
       "12          0            0             0       1\n",
       "13          1            0             0       0\n",
       "14          1            0             0       0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(processed_data['tag'], dtype=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para recordar en el orden en que se están codificando las variables, podemos guardar la lista de los encabezados de dicha tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_categories = list(pd.get_dummies(processed_data['tag']).columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, para reutilizar posteriormente dicha información, salvaremos la variable `sample_categories` en formato pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    sample_categories,\n",
    "    open('sample_categories.pkl', 'wb')\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.4 Entrenamiento del Modelo\n",
    "\n",
    "Ahora entrenaremos un modelo con Tensorflow, tomando como características al dataframe que tiene la información de la bolsa de palabras y como variable objetivo a la representación en One Hot Encoding de cada categoria asociada a las preguntas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension de datos de entrada (nube de palabras)\n",
    "dim_x = len(processed_data._get_numeric_data().to_numpy()[0])\n",
    "\n",
    "# dimension de la representacion One Hot Encoding\n",
    "dim_y = len(pd.get_dummies(processed_data['tag'], dtype=int).to_numpy()[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un modelo en Tensorflow con estas características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(25, input_shape=(dim_x,), activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(dim_y, activation='softmax')\n",
    "]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora compilamos el modelo espeficando su función de pérdida, optimizador y métrica para medir su performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entreamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bolsa de palabras como arreglo de numpy\n",
    "X_train = processed_data._get_numeric_data().to_numpy()\n",
    "\n",
    "# Representación en One Hot Encoding de las categorias\n",
    "y_train = pd.get_dummies(processed_data['tag'], dtype=float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 1.4498 - accuracy: 0.2667\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4700 - accuracy: 0.0667\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4334 - accuracy: 0.2000\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4759 - accuracy: 0.2000\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4138 - accuracy: 0.4000\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4300 - accuracy: 0.2000\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.4125 - accuracy: 0.4667\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4439 - accuracy: 0.1333\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4272 - accuracy: 0.2000\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4159 - accuracy: 0.1333\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3773 - accuracy: 0.3333\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4035 - accuracy: 0.2667\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.4199 - accuracy: 0.2000\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4485 - accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3917 - accuracy: 0.2667\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4406 - accuracy: 0.2000\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4046 - accuracy: 0.3333\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3865 - accuracy: 0.2667\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3941 - accuracy: 0.1333\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3955 - accuracy: 0.0667\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4164 - accuracy: 0.1333\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3602 - accuracy: 0.3333\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4001 - accuracy: 0.3333\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3807 - accuracy: 0.4000\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4069 - accuracy: 0.1333\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3950 - accuracy: 0.4667\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4006 - accuracy: 0.2667\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4141 - accuracy: 0.2000\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3454 - accuracy: 0.1333\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4050 - accuracy: 0.5333\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3688 - accuracy: 0.4667\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3921 - accuracy: 0.4000\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3553 - accuracy: 0.3333\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3821 - accuracy: 0.2667\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3968 - accuracy: 0.2000\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3088 - accuracy: 0.4667\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3854 - accuracy: 0.4000\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4392 - accuracy: 0.2667\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3745 - accuracy: 0.1333\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3856 - accuracy: 0.3333\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4109 - accuracy: 0.2667\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4058 - accuracy: 0.2667\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3958 - accuracy: 0.1333\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3684 - accuracy: 0.4667\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3559 - accuracy: 0.2667\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3789 - accuracy: 0.3333\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3531 - accuracy: 0.3333\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3592 - accuracy: 0.2667\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3921 - accuracy: 0.1333\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3931 - accuracy: 0.1333\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3621 - accuracy: 0.2667\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3094 - accuracy: 0.4000\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3496 - accuracy: 0.3333\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3498 - accuracy: 0.2667\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3967 - accuracy: 0.0667\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3428 - accuracy: 0.3333\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3405 - accuracy: 0.4000\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3460 - accuracy: 0.1333\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2894 - accuracy: 0.3333\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3317 - accuracy: 0.3333\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3537 - accuracy: 0.2000\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2897 - accuracy: 0.4667\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3436 - accuracy: 0.2667\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2967 - accuracy: 0.3333\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3581 - accuracy: 0.2667\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3587 - accuracy: 0.3333\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3353 - accuracy: 0.3333\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3140 - accuracy: 0.5333\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3331 - accuracy: 0.2667\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3420 - accuracy: 0.3333\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3285 - accuracy: 0.3333\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3592 - accuracy: 0.2667\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3034 - accuracy: 0.3333\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3440 - accuracy: 0.4000\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3246 - accuracy: 0.5333\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3175 - accuracy: 0.4000\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2872 - accuracy: 0.5333\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3733 - accuracy: 0.2667\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.3246 - accuracy: 0.4000\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3197 - accuracy: 0.4667\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3830 - accuracy: 0.2667\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2991 - accuracy: 0.4000\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2679 - accuracy: 0.4667\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3364 - accuracy: 0.2667\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3352 - accuracy: 0.3333\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3223 - accuracy: 0.3333\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3051 - accuracy: 0.4000\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3082 - accuracy: 0.2667\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3019 - accuracy: 0.4000\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2863 - accuracy: 0.2000\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2739 - accuracy: 0.6000\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3084 - accuracy: 0.3333\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2933 - accuracy: 0.4000\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2967 - accuracy: 0.4000\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2579 - accuracy: 0.5333\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2944 - accuracy: 0.4667\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2872 - accuracy: 0.5333\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2604 - accuracy: 0.5333\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3098 - accuracy: 0.4667\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2655 - accuracy: 0.3333\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2446 - accuracy: 0.4667\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3258 - accuracy: 0.4000\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3405 - accuracy: 0.4000\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2851 - accuracy: 0.3333\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2038 - accuracy: 0.5333\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2781 - accuracy: 0.4667\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2745 - accuracy: 0.4667\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2575 - accuracy: 0.6000\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2386 - accuracy: 0.3333\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2638 - accuracy: 0.5333\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2841 - accuracy: 0.4667\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.2532 - accuracy: 0.5333\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2729 - accuracy: 0.3333\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2731 - accuracy: 0.4000\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2744 - accuracy: 0.4667\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2340 - accuracy: 0.6000\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3084 - accuracy: 0.4000\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2815 - accuracy: 0.3333\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2701 - accuracy: 0.5333\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2473 - accuracy: 0.4000\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2930 - accuracy: 0.3333\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2390 - accuracy: 0.3333\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2862 - accuracy: 0.6000\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2834 - accuracy: 0.4667\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2762 - accuracy: 0.4000\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2512 - accuracy: 0.6000\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3142 - accuracy: 0.3333\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2615 - accuracy: 0.6000\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2198 - accuracy: 0.4000\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2225 - accuracy: 0.4667\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2219 - accuracy: 0.5333\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2231 - accuracy: 0.5333\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2156 - accuracy: 0.6667\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2295 - accuracy: 0.7333\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2527 - accuracy: 0.4667\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2472 - accuracy: 0.5333\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2845 - accuracy: 0.4667\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2380 - accuracy: 0.6000\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2697 - accuracy: 0.3333\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2350 - accuracy: 0.5333\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2478 - accuracy: 0.4667\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1938 - accuracy: 0.5333\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2886 - accuracy: 0.4000\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2297 - accuracy: 0.4667\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2608 - accuracy: 0.3333\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2398 - accuracy: 0.5333\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1951 - accuracy: 0.4667\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2349 - accuracy: 0.4667\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2076 - accuracy: 0.5333\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2436 - accuracy: 0.5333\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1823 - accuracy: 0.5333\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1468 - accuracy: 0.6000\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2482 - accuracy: 0.4667\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2148 - accuracy: 0.4667\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2718 - accuracy: 0.4667\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2447 - accuracy: 0.4667\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2104 - accuracy: 0.6667\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2456 - accuracy: 0.4000\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.2619 - accuracy: 0.6000\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2170 - accuracy: 0.4000\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1993 - accuracy: 0.4000\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2381 - accuracy: 0.4000\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1990 - accuracy: 0.6667\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1701 - accuracy: 0.6000\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2061 - accuracy: 0.6000\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2269 - accuracy: 0.4000\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2015 - accuracy: 0.6667\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2595 - accuracy: 0.4000\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2059 - accuracy: 0.7333\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2417 - accuracy: 0.2667\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1832 - accuracy: 0.5333\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1800 - accuracy: 0.7333\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1553 - accuracy: 0.6667\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2375 - accuracy: 0.3333\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2063 - accuracy: 0.4667\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2705 - accuracy: 0.4667\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2206 - accuracy: 0.5333\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.2307 - accuracy: 0.5333\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1677 - accuracy: 0.7333\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2743 - accuracy: 0.4000\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1919 - accuracy: 0.5333\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1905 - accuracy: 0.7333\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1975 - accuracy: 0.5333\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2192 - accuracy: 0.4667\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1856 - accuracy: 0.5333\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1613 - accuracy: 0.7333\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2110 - accuracy: 0.4667\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1864 - accuracy: 0.4667\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2034 - accuracy: 0.4667\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2182 - accuracy: 0.4000\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1634 - accuracy: 0.6667\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1680 - accuracy: 0.5333\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1782 - accuracy: 0.5333\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1749 - accuracy: 0.6000\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1562 - accuracy: 0.5333\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1990 - accuracy: 0.6000\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2064 - accuracy: 0.6667\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1640 - accuracy: 0.6000\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2015 - accuracy: 0.6667\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1951 - accuracy: 0.5333\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1809 - accuracy: 0.6000\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1405 - accuracy: 0.7333\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1711 - accuracy: 0.8000\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1323 - accuracy: 0.6000\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2063 - accuracy: 0.5333\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1623 - accuracy: 0.7333\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2151 - accuracy: 0.5333\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1208 - accuracy: 0.5333\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1302 - accuracy: 0.6000\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1189 - accuracy: 0.5333\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0917 - accuracy: 0.7333\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1436 - accuracy: 0.6000\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1163 - accuracy: 0.6667\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1618 - accuracy: 0.4667\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1895 - accuracy: 0.6667\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1810 - accuracy: 0.5333\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1254 - accuracy: 0.8000\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1207 - accuracy: 0.7333\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1945 - accuracy: 0.6667\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1345 - accuracy: 0.8667\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1359 - accuracy: 0.6667\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1834 - accuracy: 0.4667\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1730 - accuracy: 0.6000\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1142 - accuracy: 0.6667\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0642 - accuracy: 0.6000\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1209 - accuracy: 0.5333\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1031 - accuracy: 0.6000\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1096 - accuracy: 0.5333\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1044 - accuracy: 0.7333\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1199 - accuracy: 0.6667\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1695 - accuracy: 0.6667\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1560 - accuracy: 0.6667\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0891 - accuracy: 0.6000\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1356 - accuracy: 0.6667\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1464 - accuracy: 0.6667\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0974 - accuracy: 0.6000\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0944 - accuracy: 0.6667\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0683 - accuracy: 0.9333\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0580 - accuracy: 0.8000\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0930 - accuracy: 0.8000\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0926 - accuracy: 0.6667\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0788 - accuracy: 0.8000\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1015 - accuracy: 0.6667\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1593 - accuracy: 0.6667\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1026 - accuracy: 0.8000\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1453 - accuracy: 0.4667\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0860 - accuracy: 0.6667\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1039 - accuracy: 0.5333\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.1338 - accuracy: 0.7333\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0554 - accuracy: 0.6667\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0604 - accuracy: 0.7333\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1655 - accuracy: 0.4667\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0665 - accuracy: 0.8667\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0776 - accuracy: 0.8667\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0903 - accuracy: 0.6667\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1297 - accuracy: 0.7333\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0904 - accuracy: 0.7333\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0541 - accuracy: 0.6667\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0994 - accuracy: 0.7333\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1345 - accuracy: 0.8667\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0599 - accuracy: 0.7333\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1313 - accuracy: 0.5333\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1156 - accuracy: 0.6667\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0958 - accuracy: 0.6000\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1473 - accuracy: 0.6667\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0785 - accuracy: 0.6000\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0920 - accuracy: 0.7333\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0457 - accuracy: 0.4667\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9825 - accuracy: 0.8667\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9887 - accuracy: 0.9333\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0881 - accuracy: 0.7333\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9906 - accuracy: 0.8667\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0606 - accuracy: 0.8000\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0069 - accuracy: 0.8667\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0480 - accuracy: 0.6667\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1178 - accuracy: 0.6667\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0582 - accuracy: 0.8000\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0089 - accuracy: 0.8000\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0761 - accuracy: 0.6667\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0260 - accuracy: 0.8000\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0053 - accuracy: 0.7333\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0031 - accuracy: 0.8667\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0047 - accuracy: 0.8667\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0775 - accuracy: 0.7333\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0384 - accuracy: 0.6667\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0097 - accuracy: 0.7333\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0706 - accuracy: 0.6000\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0169 - accuracy: 0.8667\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0323 - accuracy: 0.7333\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9920 - accuracy: 0.7333\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0177 - accuracy: 0.8000\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9658 - accuracy: 0.7333\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0454 - accuracy: 0.7333\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9981 - accuracy: 0.7333\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0197 - accuracy: 0.8000\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0028 - accuracy: 0.8000\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0103 - accuracy: 0.8000\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1059 - accuracy: 0.6667\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0129 - accuracy: 0.6667\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9288 - accuracy: 0.9333\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9994 - accuracy: 0.8000\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0360 - accuracy: 0.6000\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9735 - accuracy: 0.9333\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9822 - accuracy: 0.8667\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9898 - accuracy: 0.8000\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9941 - accuracy: 0.8667\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0049 - accuracy: 0.7333\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0590 - accuracy: 0.5333\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9618 - accuracy: 0.8000\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0121 - accuracy: 0.8000\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9412 - accuracy: 0.9333\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0524 - accuracy: 0.7333\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9510 - accuracy: 0.7333\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9197 - accuracy: 0.8667\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9382 - accuracy: 0.8667\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9207 - accuracy: 0.8667\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9265 - accuracy: 0.8667\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9085 - accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0575 - accuracy: 0.6667\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9722 - accuracy: 0.7333\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0222 - accuracy: 0.8000\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0157 - accuracy: 0.6667\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0014 - accuracy: 0.6667\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0408 - accuracy: 0.8000\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9433 - accuracy: 0.8667\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9739 - accuracy: 0.8667\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9284 - accuracy: 0.7333\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0666 - accuracy: 0.7333\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0826 - accuracy: 0.6000\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9665 - accuracy: 0.8667\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0155 - accuracy: 0.8667\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8546 - accuracy: 0.8667\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9585 - accuracy: 0.6667\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0033 - accuracy: 0.8000\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9989 - accuracy: 0.8000\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0030 - accuracy: 0.6667\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9097 - accuracy: 0.8667\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0407 - accuracy: 0.6667\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9110 - accuracy: 0.6667\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9197 - accuracy: 0.8000\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0196 - accuracy: 0.8000\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8511 - accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9054 - accuracy: 0.9333\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9307 - accuracy: 0.8000\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9574 - accuracy: 0.8000\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9570 - accuracy: 0.7333\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8737 - accuracy: 0.8000\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9163 - accuracy: 0.8000\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9682 - accuracy: 0.6000\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9000 - accuracy: 0.8667\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9242 - accuracy: 0.9333\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9429 - accuracy: 0.8000\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0374 - accuracy: 0.7333\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9012 - accuracy: 0.7333\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9042 - accuracy: 0.8667\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8317 - accuracy: 0.8000\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9140 - accuracy: 0.7333\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9135 - accuracy: 0.7333\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9590 - accuracy: 0.8000\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8646 - accuracy: 0.8000\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0057 - accuracy: 0.7333\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9502 - accuracy: 0.8000\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8604 - accuracy: 0.8667\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8707 - accuracy: 0.7333\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8892 - accuracy: 0.8000\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9730 - accuracy: 0.8667\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7681 - accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9484 - accuracy: 0.8667\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8386 - accuracy: 0.9333\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8324 - accuracy: 0.7333\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0183 - accuracy: 0.7333\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9527 - accuracy: 0.8000\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8770 - accuracy: 0.8667\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8901 - accuracy: 0.9333\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9492 - accuracy: 0.8667\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8448 - accuracy: 0.8000\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9134 - accuracy: 0.8000\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8620 - accuracy: 0.9333\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8342 - accuracy: 0.9333\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9295 - accuracy: 0.6667\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8968 - accuracy: 0.6000\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8549 - accuracy: 0.8667\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8236 - accuracy: 0.9333\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8939 - accuracy: 0.7333\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8233 - accuracy: 0.8000\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7973 - accuracy: 0.9333\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9461 - accuracy: 0.7333\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7759 - accuracy: 0.9333\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8281 - accuracy: 0.8000\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7380 - accuracy: 0.9333\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8588 - accuracy: 0.8667\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8725 - accuracy: 0.8667\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8427 - accuracy: 0.9333\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8145 - accuracy: 0.9333\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8654 - accuracy: 0.8667\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8151 - accuracy: 0.8000\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8504 - accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8662 - accuracy: 0.8000\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8556 - accuracy: 0.8000\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8883 - accuracy: 0.9333\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9011 - accuracy: 0.6000\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8877 - accuracy: 0.7333\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7900 - accuracy: 0.9333\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7998 - accuracy: 0.9333\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8327 - accuracy: 0.8667\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7520 - accuracy: 0.9333\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7450 - accuracy: 0.7333\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9212 - accuracy: 0.8000\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8154 - accuracy: 0.8667\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9531 - accuracy: 0.7333\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7927 - accuracy: 0.9333\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6970 - accuracy: 0.8667\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7661 - accuracy: 0.8667\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9112 - accuracy: 0.8000\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8283 - accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8057 - accuracy: 0.7333\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8217 - accuracy: 0.7333\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8575 - accuracy: 0.8667\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8419 - accuracy: 0.8000\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8428 - accuracy: 0.8667\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8510 - accuracy: 0.8667\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8570 - accuracy: 0.9333\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7794 - accuracy: 0.8000\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8018 - accuracy: 0.6667\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8070 - accuracy: 0.8000\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7849 - accuracy: 0.8667\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7359 - accuracy: 0.9333\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7855 - accuracy: 0.9333\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7316 - accuracy: 0.8667\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6783 - accuracy: 0.9333\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7212 - accuracy: 0.8667\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7598 - accuracy: 0.8667\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7547 - accuracy: 0.8000\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9144 - accuracy: 0.6667\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7836 - accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7852 - accuracy: 0.8667\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8724 - accuracy: 0.8000\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8300 - accuracy: 0.9333\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7672 - accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6392 - accuracy: 0.9333\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8258 - accuracy: 0.7333\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7802 - accuracy: 0.8667\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7447 - accuracy: 0.9333\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8107 - accuracy: 0.8000\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7522 - accuracy: 0.9333\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7380 - accuracy: 0.9333\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8681 - accuracy: 0.7333\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7127 - accuracy: 0.8667\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7778 - accuracy: 0.8667\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7234 - accuracy: 0.8667\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6957 - accuracy: 0.9333\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8400 - accuracy: 0.8000\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8187 - accuracy: 0.8000\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8166 - accuracy: 0.9333\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7989 - accuracy: 0.8000\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6881 - accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7142 - accuracy: 0.9333\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8200 - accuracy: 0.9333\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7678 - accuracy: 0.7333\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8361 - accuracy: 0.9333\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8091 - accuracy: 0.8667\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7113 - accuracy: 0.8667\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7421 - accuracy: 0.9333\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6991 - accuracy: 0.8667\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7432 - accuracy: 0.8000\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7318 - accuracy: 0.8000\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8244 - accuracy: 0.8000\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7100 - accuracy: 0.8667\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6942 - accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6879 - accuracy: 0.8667\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6637 - accuracy: 0.9333\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7759 - accuracy: 0.9333\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6666 - accuracy: 0.8000\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6462 - accuracy: 0.9333\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8604 - accuracy: 0.8667\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8715 - accuracy: 0.8000\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6518 - accuracy: 0.9333\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7246 - accuracy: 0.8667\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6354 - accuracy: 0.9333\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7245 - accuracy: 0.8000\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7807 - accuracy: 0.6667\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7092 - accuracy: 0.8667\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6886 - accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6268 - accuracy: 0.9333\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8036 - accuracy: 0.7333\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6688 - accuracy: 0.8667\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7975 - accuracy: 0.8000\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5351 - accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7402 - accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6280 - accuracy: 0.9333\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7371 - accuracy: 0.8000\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6865 - accuracy: 0.9333\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.6232 - accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6635 - accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7219 - accuracy: 0.9333\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6568 - accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6502 - accuracy: 0.9333\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.8667\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6263 - accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7216 - accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=100,\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    model,\n",
    "    open('sample_model.pkl', 'wb')\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.5 Prediciendo la respuesta para un nuevo mensaje\n",
    "\n",
    "Supongamos ahora que queremos obtener la respuesta para el nuevo mensaje *\"este, como te llamas?\"*.\n",
    "\n",
    "El proceso que debemos seguir es exactamente el mismo, limpiar el texto, crear la representación numérica del mensaje, tal como se uso en el entrenamiento del modelo, aplicar el modelo a dicha representación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = \"este?, ¿como te llamas!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pre_process(message: str):\n",
    "    \"\"\"\n",
    "    Procesa el texto del nuevo mensaje\n",
    "    \"\"\"\n",
    "    # Procesa el mensaje con spaCy\n",
    "    tokens = nlp(message)\n",
    "    # remueve signos de puntuacion y lematiza\n",
    "    new_tokens = [t.orth_ for t in tokens if not  t.is_punct]\n",
    "\n",
    "    # pasa a minusculas\n",
    "    new_tokens = [t.lower() for t in new_tokens]\n",
    "\n",
    "    # une los tokens procesados con un espacio\n",
    "    clean_message = ' '.join(new_tokens)\n",
    "    \n",
    "    return clean_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'este como te llamas'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pre_process(new_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_representation(message: str)-> np.array:\n",
    "    \"\"\"\n",
    "    Obtiene la representacion del mensaje en su\n",
    "    codificacion de la nube de palabras\n",
    "    \"\"\"\n",
    "    bow_message = vectorizer.transform(\n",
    "        [message]\n",
    "        ).toarray()\n",
    "\n",
    "    return bow_message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estas funciones podemos representar numericamente el texto limpio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# representacion del texto en la bolsa de palabras\n",
    "bow_representation(text_pre_process(new_message))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos obtener la predicción. En este punto tenemos que mencionar que el modelo regresará un vector numérico de dimensión cuatro. Aquí, como regla de dedo, se asociará el mensaje con el score más alto que tenga dichas entradas, como un proxi de que el contenido del mensaje es más afin a dicha categoria de respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07190792, 0.02794844, 0.7023362 , 0.19780739]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(bow_representation(text_pre_process(new_message))).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sabemos la entradas se encuentran codificadas con respecto al orden de la lista `sample_categories`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despedida', 'explicacion', 'introduccion', 'saludo']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, a partir de tal categoría podemos asociar una respuesta al mensaje, que es la tarea que debe ejecutar el ChatBot conversacional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(\n",
    "        bow_message: np.array\n",
    "        )-> int:\n",
    "    \"\"\"\n",
    "    Obtiene la prediccion de la categoria\n",
    "    que corresponde al mensaje\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcula el indice entero al que corresponde la catforia\n",
    "    prediction = model(bow_message).numpy()\n",
    "\n",
    "    # Obtiene el indice de la entrada con probabilidad mayor\n",
    "    index = np.argmax(prediction)\n",
    "\n",
    "    predicted_category = sample_categories[index] \n",
    "\n",
    "    return predicted_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'introduccion'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\n",
    "    bow_representation(\n",
    "        text_pre_process(new_message)\n",
    "        )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despedida', 'explicacion', 'introduccion', 'saludo']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(category: str)-> str:\n",
    "    \"\"\"\n",
    "    Obtiene el mensaje de respuesta para una categoria\n",
    "    \"\"\"\n",
    "    # Obtiene las respuestas de la categoria\n",
    "    answers = category_answers[category]\n",
    "\n",
    "    # Selecciona una respuesta al azar\n",
    "    ans = random.choice(answers)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el código que nos da la respuesta correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_answer = get_answer(get_prediction(\n",
    "    bow_representation(\n",
    "        text_pre_process(new_message)\n",
    "    )\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un bot de conversación, el usuario vería lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuario: este?, ¿como te llamas!\n",
      "ChatBot: Soy ChatBot, soy un bot de conversación :)\n"
     ]
    }
   ],
   "source": [
    "print(\"Usuario:\", new_message)\n",
    "print(\"ChatBot:\", bot_answer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Entregables\n",
    "\n",
    "A. Modifica el código presentado anteriormente para crear un script en Python (`conversations_train.py`) que entre un modelo de Deep Learning, con el propósito de obtener mensajes de respuesta a las conversaciones especificadas en el archivo **conversations.json** provisto por el equipo de Amira Rashid, **asegurándote de remover signos de puntuacios y stop words**.\n",
    "\n",
    "Este modelo deberá tener como elemento de salida, archivos de los siguiente elementos:\n",
    "\n",
    "    * Un diccionario donde las llaves son categorías de preguntas y los valores son las lista de respuestas que el equipo de la influencer espera que el ChatBot comunique a sus usuarios, denomiando `conversations_category_answers.json`,\n",
    "    * El tranformador de Sklearn que permite transformar un mensaje en texto a la bolsa de palabras, mismo que deberá ser ajusta con el corpus de preguntas de los usuarios, que el equipo de Amira proporcinó. Este se deberá denominar `conversations_vectorizer_bow.pkl`\n",
    "    * La versión pickle de la lista de nombres de categorías que asociadas a las preguntas planteadas por el equipo de la influencer, en el orden específico en que se haya realizado el One Hot Encoding para representarlas en el procesamiento del modelo. Este se deberá denominar `conversations_categories.pkl`.\n",
    "    * El modelo entrenado para tal efecto, en versión pickle. Dicho archivo deberá denominarse `model_conversations_chatbot.pkl`\n",
    "\n",
    "B. Adicionalmente, tras su ejecución el script deberá escribir a pantalla (estandar output, en consola), el mensaje que obtendría un usuario al escribir \"Mi signo es Tauro\", como resultado de emplear el modelo entrenado para predecir que mensaje asociarle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aument_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
